{
  "_meta": {
    "saved_at": "2026-01-24T22:08:39.933231",
    "source": "gemini",
    "model": "gemini-3-pro-preview",
    "query": "How does ACI select datasets for queries? List all available analysis sets and explain the routing matrix.",
    "query_length": 106,
    "checksum": "sha256:8d885dd5ab2c0f2846a6d99eeadcaf13cffb7572c90b7d37e3f2649f32ad435a"
  },
  "response": {
    "content": "Based on the `context-management/docs/ACI_DATASET_MANAGEMENT.md` and related documentation, here is the breakdown of how the Adaptive Context Intelligence (ACI) system selects data, the available sets, and the routing logic.\n\n### 1. How ACI Selects Datasets\n\nACI treats context selection as a data engineering problem rather than a simple retrieval task. It uses a 4-step pipeline to \"compile\" a curriculum for the AI:\n\n1.  **Query Analysis (The Profiler)**\n    The system first profiles the user query to determine three factors:\n    *   **Intent:** (e.g., ARCHITECTURE, DEBUG, TASK, COUNT).\n    *   **Complexity:** Simple (<3 keywords), Moderate, or Complex (>6 keywords).\n    *   **Scope:** Internal (repo only), External (world knowledge), or Hybrid.\n\n2.  **Semantic Graph Matching (The Locator)**\n    It maps the query against the \"Standard Model of Code\" graph using 8 dimensions (Role, Layer, etc.).\n    *   It calculates **Semantic Distance** to find relevant code atoms.\n    *   It performs **Edge Traversal** (Upstream/Downstream) to find dependencies and dependents.\n    *   It determines **Context Flow**: Is the context \"Laminar\" (coherent/local) or \"Turbulent\" (scattered)?\n\n3.  **Set Resolution & Merging**\n    If the routing decision points to Long Context (Tier 2), ACI resolves named sets defined in `analysis_sets.yaml`.\n    *   **Composition:** It expands `includes` (e.g., `architecture_review` includes `pipeline` + `classifiers`).\n    *   **Token Budgeting:** It applies a `max()` aggregation strategy on token limits (not a sum) to ensure headroom.\n    *   **Positioning:** It applies a \"Sandwich Strategy,\" placing critical files at the very beginning and very end of the context window to combat the LLM \"Lost-in-Middle\" effect.\n\n4.  **Agent Context Injection**\n    If the intent is `TASK`, the system automatically injects the `.agent/` context (kernel, protocols, memory) regardless of the specific code files selected.\n\n---\n\n### 2. Available Analysis Sets\n\nBased on the configuration files and documentation references provided, the following sets are defined or referenced in the system:\n\n| Set Name | Description / Contents | Token Budget |\n| :--- | :--- | :--- |\n| **`architecture_review`** | Full architectural overview. Includes `pipeline`, `classifiers`, and `constraints`. Uses \"sandwich\" positioning. | 200k (Hard Cap) |\n| **`pipeline`** | Core analysis logic files (e.g., `full_analysis.py`). | 120k |\n| **`classifiers`** | Logic for atom/role classification (`src/core/classification/*.py`). | 80k |\n| **`constraints`** | System rules and invariants (`schema/constraints/*.yaml`). | 50k |\n| **`brain`** | AI tools and configuration (Brain hemisphere). | N/A (Dynamic) |\n| **`theory`** | Theoretical documentation (`THEORY.md`, etc.). | ~50-60k |\n| **`schema`** | JSON schemas defining atoms, roles, and dimensions. | N/A |\n| **`agent_tasks`** | Active tasks from `.agent/registry`. | N/A |\n| **`agent_kernel`** | Core agent protocols (`AGENT_KERNEL.md`). | ~2k |\n| **`agent_full`** | Complete agent context including memory and intelligence. | High |\n\n*Note: Sets can be dynamically merged. For example, ACI may generate a temporary set containing [`agent_tasks`, `agent_kernel`, `pipeline`] for a specific request.*\n\n---\n\n### 3. The Routing Matrix\n\nThe **Tier Router** determines *how* the query is answered based on the `[Intent × Complexity × Scope]` profile. This prevents wasting tokens on simple queries or under-powering complex ones.\n\n#### The Matrix Logic\n\n| Intent | Complexity | Scope | **Tier Selected** | **Mechanism** |\n| :--- | :--- | :--- | :--- | :--- |\n| **COUNT** | SIMPLE | INTERNAL | **TIER 0 (Instant)** | Returns cached facts from `repo_truths.yaml`. Zero tokens used. |\n| **LOCATE** | SIMPLE | INTERNAL | **TIER 1 (RAG)** | Performs vector/keyword search. Returns citations. (~1k tokens). |\n| **DEBUG** | MODERATE | INTERNAL | **TIER 2 (Long Context)** | Loads specific file clusters for reasoning. (50k-150k tokens). |\n| **ARCHITECTURE**| ANY | INTERNAL | **TIER 2 (Long Context)** | Loads broad structural context. (Up to 200k tokens). |\n| **TASK** | ANY | ANY | **TIER 2 (Long Context)** | Injects Agent context + relevant code. |\n| **RESEARCH** | ANY | EXTERNAL | **TIER 3 (Perplexity)** | Performs live web search via Perplexity API. |\n\n#### Tier Performance Reference\n\n*   **Tier 0 (Instant):** <100ms, $0 cost. (e.g., \"How many python files are there?\")\n*   **Tier 1 (RAG):** ~5s, Low cost. (e.g., \"Where is the `UnifiedNode` class defined?\")\n*   **Tier 2 (Long Context):** ~60s, High cost ($0.10-$0.40). (e.g., \"Refactor the semantic matcher to support 12 dimensions.\")\n*   **Tier 3 (Perplexity):** ~30s, Medium cost. (e.g., \"What are the latest best practices for tree-sitter in 2026?\")",
    "mode": "standard"
  }
}