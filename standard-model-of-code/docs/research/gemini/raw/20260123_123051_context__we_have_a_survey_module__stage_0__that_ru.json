{
  "_meta": {
    "saved_at": "2026-01-23T12:30:51.149220",
    "source": "gemini",
    "model": "gemini-3-pro-preview",
    "query": "CONTEXT: We have a Survey module (Stage 0) that runs before code analysis. Currently it's positioned as 'exclude vendor code' but we've realized it's actually ONTOLOGICAL - it DEFINES what the codome IS before we measure it.\n\nThe distinction:\n- OLD VIEW: Survey = optimization (skip vendor to save time)\n- NEW VIEW: Survey = ontological definition (define boundaries before measuring)\n\nKey insight: 'Before you measure a thing, you must define what the thing IS.'\n\nSurvey answers 5 questions:\n1. IDENTITY: What is this project? (language, framework, ecosystem)\n2. BOUNDARIES: Where does it start/end? (ours vs vendor vs generated)\n3. NATURE: What file types are code vs config vs data?\n4. POLLUTION: What shouldn't be here? (misplaced files)\n5. ADAPTATION: How should pipeline behave? (parsers, thresholds)\n\nQUESTIONS:\n1. Should we make this ontological distinction MORE CLEAR in docs and code?\n2. If yes, HOW? Concrete suggestions for:\n   - Naming (should 'Survey' be renamed to something more ontological?)\n   - Documentation structure\n   - Code organization\n   - User-facing messaging (CLI output, reports)\n3. Is the 5-question framework the right structure, or should it be different?\n4. Any risks or downsides to this framing?\n\nBe specific and actionable.",
    "query_length": 1260,
    "checksum": "sha256:dac58f5e725faee7412374225029df781302e5a4bbdef034d43d73584f3032cc"
  },
  "response": {
    "content": "Based on the codebase context provided, particularly `src/core/survey.py` and `src/core/full_analysis.py`, here is the analysis and specific recommendations for reframing the Survey module.\n\n### 1. Should we make this ontological distinction MORE CLEAR?\n\n**YES.**\n\nYour insight is architecturally sound. Currently, `src/core/survey.py` is written defensively (\"What do I exclude?\"). This is a \"Negative Definition.\" Shifting to a \"Positive Definition\" (\"What is this system?\") creates a stronger foundation for the subsequent analysis.\n\nIf Stage 0 misidentifies the system (e.g., treating a Monorepo as a single monolith, or analyzing build artifacts as source code), Stages 1-12 will hallucinate structure where none exists. **The definition of the object must precede the measurement of the object.**\n\n### 2. HOW? Concrete Suggestions\n\n#### A. Naming & Terminology (The \"Codome\" Shift)\n\nKeep the filename `survey.py` (it describes the action), but change the conceptual entities within it.\n\n*   **Phase Name:** Rename \"Stage 0: Survey\" to **\"Stage 0: Codome Definition\"**.\n*   **Result Object:** Rename `SurveyResult` to **`CodomeManifest`**.\n*   **Exclusions:** Rename `recommended_excludes` to **`BoundaryConstraint`**.\n\n#### B. Code Organization (`src/core/survey.py`)\n\nRefactor `SurveyResult` to explicitly answer the 5 questions. Currently, it mixes counts with exclusions.\n\n**Current (`src/core/survey.py`):**\n```python\n@dataclass\nclass SurveyResult:\n    root_path: str\n    scan_time_ms: float\n    total_files: int = 0\n    # ... lists of exclusions ...\n    estimated_nodes: int = 0\n```\n\n**Proposed Refactor:**\n```python\n@dataclass\nclass SystemIdentity:\n    primary_language: str      # e.g., \"Python\"\n    dominant_framework: str    # e.g., \"Django\" (detect via lockfiles in survey)\n    architecture_style: str    # e.g., \"Monorepo\" vs \"Standard\"\n\n@dataclass\nclass CodomeManifest:\n    \"\"\"Defines the ontological boundaries of the system to be analyzed.\"\"\"\n    identity: SystemIdentity   # Q1: Identity\n    \n    # Q2: Boundaries (The \"Inclusion/Exclusion\" logic)\n    boundary_constraints: List[str]  # Formerly recommended_excludes\n    \n    # Q3: Nature (Code vs Config vs Data ratios)\n    composition: Dict[str, float] # e.g., {'source': 0.8, 'config': 0.1, 'data': 0.1}\n    \n    # Q4: Pollution (Binaries, huge logs, minified files inside source dirs)\n    pollution_alerts: List[str]\n    \n    # Q5: Adaptation (Config for the next stages)\n    pipeline_config: Dict[str, Any] # Recommended parser settings\n```\n\n#### C. User-Facing Messaging (`src/core/full_analysis.py`)\n\nUpdate the CLI output in `run_full_analysis` to reflect this ontological stance. Instead of listing what you *ignored*, list what you *found*.\n\n**Current:**\n```text\nðŸ” Stage 0: Survey...\n   â†’ Found 12 vendor directories\n   â†’ Excluding 45 paths\n   â†’ Estimated 5000 nodes\n```\n\n**Proposed:**\n```text\nðŸŒ Stage 0: Codome Definition\n   â†’ Identity:   Python / FastAPI Application\n   â†’ Boundaries: Rigid (Vendor & Build artifacts excluded)\n   â†’ Nature:     85% Source Code, 15% Configuration\n   â†’ Pollution:  Clean (No binary artifacts detected)\n   â†’ Adaptation: Tuning AST parsers for Python\n```\n\n#### D. Documentation Structure\n\nUpdate the module docstring in `src/core/survey.py` to serve as the manifesto for this layer.\n\n```python\n\"\"\"\nCODOME DEFINITION LAYER (Stage 0)\n=================================\n\nThis module defines the ontology of the target system before analysis begins.\nIt is not merely an optimization (exclusion); it is a definition.\n\nIt answers:\n1. IDENTITY: What is this? (Language, Framework)\n2. BOUNDARIES: What is 'Self' vs 'Other'? (Vendor, Gen-code)\n3. NATURE: What is the texture? (Code/Config/Data ratio)\n4. POLLUTION: What violates the physics of this repo?\n5. ADAPTATION: How must the instruments be calibrated?\n\"\"\"\n```\n\n### 3. Is the 5-question framework right?\n\nYes, it is excellent because it maps directly to the subsequent pipeline stages:\n\n1.  **IDENTITY** $\\to$ Maps to **Stage 1 (AST Parsing)**. Knowing it's Python tells us which `TreeSitter` grammar to load.\n2.  **BOUNDARIES** $\\to$ Maps to **Stage 2.7 (Dimensions)**. Helps define `D4_BOUNDARY` (Internal vs External).\n3.  **NATURE** $\\to$ Maps to **Stage 5.6 (Enrichment)**. Helps distinguish `DAT` (Data) from `LOG` (Logic) atoms early.\n4.  **POLLUTION** $\\to$ Maps to **Stage 8.5 (Constraints)**. Early detection of \"Antimatter\" (e.g., a 50MB SQL dump in `src/`).\n5.  **ADAPTATION** $\\to$ Maps to **Observability**. Sets timeouts and memory limits based on size/complexity.\n\n**Refinement:**\nAdd **TOPOLOGY** as a sub-point of Identity. Is this a Monolith, a Monorepo, or a Microservice? `survey.py` can detect this by looking for multiple `package.json` or `requirements.txt` files at the root vs subdirs.\n\n### 4. Risks and Downsides\n\n1.  **\"The Map is not the Territory\" Risk:**\n    *   *Risk:* If the Survey defines the Codome incorrectly (e.g., decides a folder is \"Vendor\" when it's actually \"Legacy Core\"), that code effectively ceases to exist for the rest of the pipeline.\n    *   *Mitigation:* The Survey must be **Conservative by default, Aggressive by configuration**. Always report *what* was excluded and *why* in the final report.\n\n2.  **Performance Creep:**\n    *   *Risk:* \"Ontological Definition\" sounds expensive. If we start parsing `package.json` or reading file headers to determine \"Identity,\" Stage 0 might slow down.\n    *   *Mitigation:* Keep Stage 0 strictly **Filesystem-based** (file names, extensions, directory structure). Do not open files for deep AST parsing here. `detect_minified_files` already reads files; ensure it reads *only* headers/first lines.\n\n3.  **The \"Polyglot\" Edge Case:**\n    *   *Risk:* Defining \"Identity\" is hard for mixed repos (e.g., Python backend + React frontend).\n    *   *Mitigation:* The `Identity` struct must support lists/sets (e.g., `primary_languages: [\"Python\", \"TypeScript\"]`). Do not force a single identity.\n\n### Action Plan\n\n1.  **Modify `src/core/survey.py`**:\n    *   Rename internal variables to match the 5 questions (e.g., `exclusions` -> `boundaries`).\n    *   Add simple heuristics to detect `Identity` (look for `pom.xml`, `requirements.txt`, `package.json`).\n2.  **Modify `src/core/full_analysis.py`**:\n    *   Update the print statements for Stage 0.\n3.  **Documentation**:\n    *   Update the docstring in `survey.py`.",
    "mode": "standard"
  }
}