{
  "_meta": {
    "saved_at": "2026-01-24T19:47:51.796288",
    "source": "gemini",
    "model": "gemini-3-pro-preview",
    "query": "Audit the ACI system for correctness issues. Specifically:\n1. The HYBRID tier is defined in tier_router.py but analyze.py has no execution path for it - what happens when HYBRID is returned?\n2. The semantic_matcher.py can emit sets like 'gateways', 'infrastructure', 'services' that don't exist in analysis_sets.yaml - trace this code path\n3. The agent_context trigger_keywords in aci_config.yaml include generic words like 'confidence' and 'run' - show false positive scenarios\n4. The Perplexity integration in analyze.py - does it inject internal context into the user prompt (which Perplexity docs say pollutes search)?\n\nFor each issue, cite file:line numbers and propose a fix.",
    "query_length": 681,
    "checksum": "sha256:c4d29be1b7172ce2697bb05949e271d87177263284da6c599c5dfcdd460ddb8b"
  },
  "response": {
    "content": "Here is the correctness audit for the Adaptive Context Intelligence (ACI) system.\n\n## Executive Summary\nThe ACI system demonstrates a strong architectural vision (Graph -> Tier -> Context), but suffers from \"implementation drift\" where specific configurations and mappings have disconnected from the actual execution logic. The most critical issue is the `semantic_matcher.py` referencing non-existent analysis sets, which will cause the \"Curriculum Compiler\" to return empty contexts for semantically complex queries.\n\n---\n\n### 1. HYBRID Tier: Missing Execution Logic\n**Issue**: The `Tier.HYBRID` enum is defined and routed to, but `analyze.py` treats it as a fall-through case, effectively downgrading it to `LONG_CONTEXT` without the external research component that \"Hybrid\" implies.\n\n**Evidence**:\n*   **Definition**: `context-management/tools/ai/aci/tier_router.py:108` defines `HYBRID = \"hybrid\"`.\n*   **Routing**: `tier_router.py:209-216` assigns `Tier.HYBRID` for mixed internal/external scope queries.\n*   **Execution Gap**: `context-management/tools/ai/analyze.py:1618` (approximate).\n    The execution flow handles `INSTANT` (Line 1391), `PERPLEXITY` (Line 1420), and `FLASH_DEEP` (Line 1500). There is no `if decision.tier == Tier.HYBRID:` block. The code falls through to the catch-all block (\"TIER 1 & 2\") at Line 1618.\n\n**Impact**: Queries needing external validation (e.g., \"Compare our architecture vs 2026 industry standards\") will only search the local codebase, failing to retrieve the \"industry standard\" part.\n\n**Proposed Fix**:\nModify `context-management/tools/ai/analyze.py` to intercept `Tier.HYBRID`. A simple implementation runs Perplexity first, then injects that result into the standard context context.\n\n```python\n# In analyze.py, before the generic TIER 1 & 2 block\n\nif decision.tier == Tier.HYBRID:\n    print(\"[HYBRID] Executing external research phase...\", file=sys.stderr)\n    # 1. Run Perplexity\n    try:\n        from perplexity_research import research\n        # Use Gemini to extract the external part of the query?\n        # For now, send full query\n        research_res = research(args.prompt, model=\"sonar-pro\")\n        external_context = f\"\\n=== EXTERNAL RESEARCH ===\\n{research_res['content']}\\n\"\n    except Exception as e:\n        print(f\"[HYBRID] Research failed ({e}), proceeding with internal only.\", file=sys.stderr)\n        external_context = \"\"\n    \n    # 2. Proceed to Internal Analysis (fall through but append context)\n    # We need a mechanism to inject this into the context builder later\n    # This requires refactoring the context build logic to accept string injections\n```\n\n---\n\n### 2. Semantic Matcher: \"Hallucinated\" Analysis Sets\n**Issue**: The semantic matcher maps semantic intents to analysis sets that do not exist in the configuration. The `resolve_set` function in `analyze.py` will return empty patterns for these, resulting in zero context loaded.\n\n**Evidence**:\n*   **Source**: `context-management/tools/ai/aci/semantic_matcher.py`\n    *   Line 354: `\"Retrieve\": [\"pipeline\", \"data_access\"]` -> `data_access` does not exist.\n    *   Line 355: `\"Persist\": [\"infrastructure\", \"data_access\"]` -> `infrastructure` does not exist.\n    *   Line 359: `\"Coordinate\": [\"services\", \"orchestration\"]` -> `services`, `orchestration` do not exist.\n    *   Line 360: `\"Transport\": [\"gateways\", \"infrastructure\"]` -> `gateways` does not exist.\n    *   Line 361: `\"Present\": [\"presentation\", \"viz_core\"]` -> `presentation` does not exist.\n    *   Lines 368-372: `controllers`, `api`, `domain`, `entities`, `validators` do not exist.\n*   **Config**: `context-management/config/analysis_sets.yaml` contains `pipeline`, `viz_core`, `classifiers`, `repo_tool`, etc., but matches very few of the semantic suggestions.\n\n**Impact**: High-semantic queries (e.g., \"How do we coordinate services?\") will be routed to sets like `['services', 'orchestration']`. Since these sets are undefined, the AI will receive **empty context** or fallback to defaults, rendering the \"Curriculum Compiler\" useless.\n\n**Proposed Fix**:\nRemap `semantic_matcher.py` to use existing sets defined in `analysis_sets.yaml`.\n\n```python\n# context-management/tools/ai/aci/semantic_matcher.py\n\ndef _purpose_to_sets(purpose: str) -> List[str]:\n    mapping = {\n        \"Retrieve\": [\"pipeline\", \"repo_tool\"],       # 'repo_tool' contains engine logic\n        \"Persist\": [\"repo_tool\", \"schemas\"],         # 'schemas' defines data structures\n        \"Transform\": [\"pipeline\", \"classifiers\"],    # Existing sets\n        \"Compute\": [\"pipeline\", \"repo_theory\"],\n        \"Validate\": [\"classifiers\", \"constraints\"],  # 'constraints' exists\n        \"Coordinate\": [\"pipeline\", \"agent_tasks\"],   # 'agent_tasks' for coordination\n        \"Transport\": [\"pipeline\"],                   # Fallback\n        \"Present\": [\"viz_core\"],                     # Exists\n    }\n    return mapping.get(purpose, [\"pipeline\"])\n\n# Similarly update _layer_to_sets to use 'repo_theory', 'repo_tool', 'classifiers'\n```\n\n---\n\n### 3. Agent Context: High False-Positive Rate\n**Issue**: The trigger keywords for injecting the Agent/Kernel context are too generic. Common English words trigger a context injection of ~30k-50k tokens (`agent_kernel` + `agent_tasks`), wasting budget and distracting the model.\n\n**Evidence**:\n*   **Config**: `context-management/config/aci_config.yaml:80`\n    *   `- \"confidence\"` (Used in \"I have low confidence in this code\")\n    *   `- \"run\"` (Used in \"Run the script\", \"Runtime error\")\n    *   `- \"task\"` (Used in \"The task failed\")\n*   **Logic**: `context-management/tools/ai/aci/query_analyzer.py:344`\n    ```python\n    for indicator in agent_indicators:\n        if indicator in query_lower:\n            return True\n    ```\n    This is a simple substring check. \"runtime error\" contains \"run\". \"verify task execution\" contains \"task\".\n\n**Impact**:\n1.  **Cost**: Unnecessary loading of `agent_tasks` (50k tokens) for simple debugging queries.\n2.  **Confusion**: The model sees Sprint planning docs when asked to debug a Python script.\n\n**Proposed Fix**:\nRefine `aci_config.yaml` to use more specific compound terms, or switch `query_analyzer.py` to use token-based matching rather than substring matching.\n\n**Immediate Config Fix (`aci_config.yaml`):**\n```yaml\n  trigger_keywords:\n    - \"agent task\"\n    - \"current sprint\"\n    - \"kernel boot\"\n    - \"bare protocol\"\n    - \"update registry\"\n    - \"confidence score\" # Specific to the meta-metric\n    - \"claim task\"\n    # Remove \"run\", \"confidence\", \"task\" (too generic)\n```\n\n---\n\n### 4. Perplexity Integration: Context Pollution\n**Issue**: The logic in `analyze.py` injects raw local code excerpts into the prompt sent to Perplexity. Search engine LLMs (Sonar) are optimized for web search queries; injecting 2000 chars of local Python code often confuses the retrieval mechanism, causing it to search for the *specific code strings* on the web rather than the *concept*.\n\n**Evidence**:\n*   **File**: `context-management/tools/ai/analyze.py`\n*   **Function**: `enrich_query_for_perplexity` (Line 538)\n*   **Logic**:\n    ```python\n    # Line 587\n    file_excerpts.append(f\"[{fname}]: {excerpt}...\")\n    # Line 600\n    enriched = f\"\"\"CONTEXT FROM LOCAL CODEBASE... \\n{...}\\n RESEARCH QUESTION: {query}\"\"\"\n    ```\n*   **Usage**: In `tier_router.py`, fallback from specific grounding often hits `enrich_query_for_perplexity`.\n\n**Impact**: A query like \"best practice for Python imports\" becomes \"best practice for Python imports... [file: pipeline.py] import os...\". Perplexity might try to find that specific file online or halluncinate that the provided code is the \"best practice\" it should reference.\n\n**Proposed Fix**:\nIn `analyze.py`, restrict `enrich_query_for_perplexity` to only inject **Definitions** (from Glossary/Theory), not **Implementation** (Code excerpts).\n\n```python\n# context-management/tools/ai/analyze.py\n\ndef enrich_query_for_perplexity(query: str, decision, analysis_sets: dict) -> str:\n    # ... (Keep glossary loading) ...\n\n    # REMOVE lines 570-594 (Critical file excerpt loading)\n    \n    # Replace with strict conceptual framing\n    return f\"\"\"\n    Context: I am working on a project called \"Standard Model of Code\" \n    (a meta-language for software architecture).\n    \n    Local Definitions relevant to query:\n    {chr(10).join(relevant_terms[:5])}\n    \n    Research Question:\n    {query}\n    \"\"\"\n```",
    "mode": "standard"
  }
}