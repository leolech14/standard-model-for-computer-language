{
  "_meta": {
    "saved_at": "2026-01-27T08:49:37.245904",
    "source": "gemini",
    "model": "gemini-3-pro-preview",
    "query": "CRITICAL ARCHITECTURE VALIDATION\n\nWe are about to build:\n1. MINIMAL HUB - Module registry + DI + event bus (~300 lines)\n2. PLUGIN SYSTEM - plugin.json manifests + lifecycle hooks\n3. MCP INTEGRATION - Model Context Protocol servers for AI agents\n4. SCHEMA VALIDATION - JSON Schema for all module contracts\n5. MULTI-CHANNEL DISTRIBUTION - Copy/paste, npm, git, CDN\n\nBEFORE we build, search the codebase and validate:\n\nQUESTION 1: Do we ALREADY HAVE a hub-like system?\n- Look for: registry patterns, service locators, dependency injection\n- Check: .agent/tools/, src/core/, context-management/\n- Files to examine: Any containing 'registry', 'hub', 'container', 'inject'\n\nQUESTION 2: Do we ALREADY HAVE module/plugin infrastructure?\n- Look for: plugin loaders, module discovery, dynamic imports\n- Check: trigger_engine.py, macro_executor.py, enrichment_orchestrator.py\n- These already load YAML configs and execute dynamically - is that our plugin system?\n\nQUESTION 3: Do we ALREADY HAVE schema validation?\n- Look for: JSON Schema validation, contract testing, type checking\n- Check: schema/ directory, validation tools\n- Files: particle.schema.json, any validators\n\nQUESTION 4: Do we ALREADY HAVE event bus / pub-sub?\n- Look for: event emission, listeners, observers, callbacks\n- Check: viz/assets/modules/event-bus.js (we have this!)\n- Is it used? Is it the pattern to expand?\n\nQUESTION 5: What SHOULD we integrate with vs rebuild?\n- If we have 70% of a plugin system already (macros, triggers, registries)\n- Should we REFACTOR existing â†’ plugin pattern?\n- Or build new hub and MIGRATE existing tools to it?\n\nPROVIDE FORENSIC CITATIONS:\n- File paths and line numbers for all evidence\n- Exact code showing existing patterns\n- Gaps where functionality is missing\n- Integration points where new hub would connect\n\nVERDICT REQUIRED:\n- BUILD NEW (nothing exists that serves this purpose)\n- REFACTOR EXISTING (we have pieces, unify them)\n- EXTEND EXISTING (build on what works, add missing pieces)\n- INTEGRATE (connect existing systems that don't talk)\n",
    "query_length": 2045,
    "checksum": "sha256:31bc89cc808c946db4f193a4a646c7e8c62774c8bebee84273e87b42bbabc450"
  },
  "response": {
    "content": "Based on the forensic analysis of the provided codebase context, here is the validation report regarding the proposed \"Minimal Hub\" architecture.\n\n### EXECUTIVE SUMMARY\n\n**VERDICT: REFACTOR & EXTEND EXISTING**\n\nThe codebase **already contains** a functional Hub (`RegistryOfRegistries`), a Plugin System (`PipelineManager`/`BaseStage`), and partial Schema Validation (`validate_contract`). Building a *new* \"Minimal Hub\" from scratch would create architectural redundancy and technical debt. The correct path is to promote `RegistryOfRegistries` to the top-level Hub and generalize the `PipelineManager` pattern to handle non-analysis plugins.\n\n---\n\n### FORENSIC EVIDENCE\n\n#### QUESTION 1: Do we ALREADY HAVE a hub-like system?\n**YES. It is explicitly defined as `RegistryOfRegistries`.**\n\n*   **Evidence:** `src/core/registry/registry_of_registries.py` implements a singleton Meta-Registry that manages other registries.\n    *   **L50-L51:** `class RegistryOfRegistries:`\n    *   **L63-L84:** Registers Atoms, Roles, Patterns, Schemas, Workflows, and Types.\n    *   **L93:** `def register(self, name: str, registry: Any) -> None:`\n    *   **L97:** `def get(self, name: str) -> Optional[Any]:`\n\n*   **Service Locator Pattern:** The codebase relies heavily on Singleton accessors acting as service locators.\n    *   `[src/core/registry/registry_of_registries.py:L127-L129]`: `get_meta_registry()`\n    *   `[src/core/type_registry.py:L137-L139]`: `get_registry()`\n    *   `[src/core/profile_loader.py:L162-L167]`: `get_profile_loader()`\n\n**Conclusion:** You have a Hub. It just needs to be exposed as the central entry point for the new features (MCP, Multi-channel).\n\n#### QUESTION 2: Do we ALREADY HAVE module/plugin infrastructure?\n**YES. The `pipeline` package is a sophisticated plugin system.**\n\n*   **Plugin Interface:** `src/core/pipeline/base_stage.py` defines the contract.\n    *   **L17:** `class BaseStage(ABC):`\n    *   **L42:** `def execute(self, state: \"CodebaseState\") -> \"CodebaseState\":`\n    *   **L54:** `def validate_input(self, state: \"CodebaseState\") -> bool:`\n\n*   **Plugin Orchestrator:** `src/core/pipeline/manager.py` handles execution.\n    *   **L18:** `class PipelineManager:`\n    *   **L50:** `def run(self, state: \"CodebaseState\") -> \"CodebaseState\":`\n\n*   **Plugin Registration:** `src/core/pipeline/stages/__init__.py` registers 27 distinct stages.\n    *   **L135:** `all_stages = { ... }` mapping string IDs to Stage instances.\n\n**Conclusion:** You do not need to build a new plugin system. You need to generalize `BaseStage` to allow for non-analysis plugins (e.g., \"DistributionStage\" or \"MCPServerStage\") or reuse the pattern.\n\n#### QUESTION 3: Do we ALREADY HAVE schema validation?\n**PARTIALLY. Contract validation exists but is scattered.**\n\n*   **Output Contract:** `src/core/normalize_output.py` enforces the output schema.\n    *   **L323:** `def validate_contract(data: Dict[str, Any]) -> Tuple[List[str], List[str]]:` checks required fields like `meta`, `nodes`, `edges`.\n\n*   **Configuration Validation:** `src/core/profile_loader.py` validates YAML structure.\n    *   **L106-L114:** Validates architecture profile structure manually during load.\n\n*   **Role Validation:** `src/core/registry/role_registry.py` enforces canonical roles.\n    *   **L37-L48:** Defines `_CANONICAL_ROLES`.\n    *   **L156:** `def is_canonical(self, role: str) -> bool:`\n\n**Conclusion:** You lack a centralized JSON Schema validator for *all* module contracts, but the logic for validation exists in `normalize_output.py`. This should be centralized into the Hub.\n\n#### QUESTION 4: Do we ALREADY HAVE event bus / pub-sub?\n**WEAK. It exists as linear hooks, not a true event bus.**\n\n*   **Hooks:** `src/core/pipeline/manager.py` supports callbacks.\n    *   **L33-L34:** `on_stage_start`, `on_stage_complete` callbacks.\n    *   **L62:** `self._on_stage_start(stage)`\n    *   **L80:** `self._on_stage_complete(stage, elapsed_ms)`\n\n*   **Implicit Events:** `src/core/registry/role_registry.py` mentions `EventBus` and `EventHandler` as roles in the analyzed code (`L172-L173` in `standard_model_enricher.py`), but the *core infrastructure* itself uses a linear pipeline, not an event loop.\n\n**Conclusion:** This is the only missing core component. The existing hooks are synchronous and coupled. A true Event Bus would decouple the stages further.\n\n#### QUESTION 5: What SHOULD we integrate with vs rebuild?\n\n**1. The Hub (Registry)**\n*   **Action:** **REFACTOR** `src/core/registry/registry_of_registries.py`.\n*   **Plan:** Rename to `CoreHub`. Add `get_service()` and `register_service()` methods to formalize DI. Use this to bootstrap MCP.\n\n**2. Plugin System**\n*   **Action:** **EXTEND** `src/core/pipeline/`.\n*   **Plan:** The `PipelineManager` is robust. Don't rebuild it. Just create a `DistributionPipeline` using the same `PipelineManager` class but with different stages (e.g., `NpmPublishStage`, `GitPushStage`).\n\n**3. Schema Validation**\n*   **Action:** **EXTEND** `src/core/normalize_output.py`.\n*   **Plan:** Extract `validate_contract` into a generic `SchemaValidator` service registered in the Hub.\n\n**4. MCP Integration**\n*   **Action:** **BUILD NEW**.\n*   **Plan:** Build `src/interface/mcp_server.py`. It should import `get_meta_registry()` (the Hub) to access tools and data.\n\n---\n\n### ARCHITECTURE DEBT WARNING\nYou have a distinct \"God Class\" risk forming in `src/core/unified_analysis.py` (L190 `create_unified_output`) and `src/core/full_analysis.py`. These files manually orchestrate the flow that `PipelineManager` was designed to handle.\n\n*   **Evidence:** `src/core/full_analysis.py` L167-L208 manually manages stages like `Survey`, `Base Analysis`, etc., despite `PipelineManager` existing.\n*   **Recommendation:** Move the logic from `full_analysis.py` into a concrete `StandardAnalysisPipeline` configuration using `PipelineManager`.\n\n### FINAL VERDICT\n\n**DO NOT BUILD FROM SCRATCH.**\n\n1.  **Hub:** Use `src/core/registry/registry_of_registries.py`.\n2.  **Plugins:** Use `src/core/pipeline/manager.py`.\n3.  **Validation:** Refactor `src/core/normalize_output.py` into a service.\n4.  **Events:** Build `src/core/event_bus.py` (New) and register it in the Hub.",
    "mode": "forensic"
  }
}