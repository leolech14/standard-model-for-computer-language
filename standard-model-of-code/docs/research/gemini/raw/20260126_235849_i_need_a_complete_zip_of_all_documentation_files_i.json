{
  "_meta": {
    "saved_at": "2026-01-26T23:58:49.054127",
    "source": "gemini",
    "model": "gemini-3-pro-preview",
    "query": "I need a complete zip of ALL documentation files in the repository - every .md, .yaml config, and schema file - not just orientation files. What's the best approach?",
    "query_length": 165,
    "checksum": "sha256:a0d39144a927e27c9601e5b2ea359d9e351bba2e6c7bff3f604e5aa4914c4fc8"
  },
  "response": {
    "content": "Based on the codebase context (specifically `TOPOLOGY_MAP.md` and `CLEANUP_PLAN.md`), you are asking to export the **Contextome** (all non-executable content) while excluding the massive generated data files (like the 800MB `unified_analysis.json`).\n\nThere is no pre-existing script for this specific \"full context\" dump (the existing `sync-orientation-files.sh` is too narrow).\n\nHere is the best approach using standard tools to create a clean **Contextome Bundle**:\n\n### The Command\n\nRun this from the root of `PROJECT_elements`. It finds all documentation, config, and schema files while aggressively excluding build artifacts, python environments, and massive analysis outputs.\n\n```bash\nfind . -type f \\\n    \\( -name \"*.md\" -o -name \"*.yaml\" -o -name \"*.json\" \\) \\\n    -not -path \"*/.git/*\" \\\n    -not -path \"*/.venv/*\" \\\n    -not -path \"*/.tools_venv/*\" \\\n    -not -path \"*/node_modules/*\" \\\n    -not -path \"*/__pycache__/*\" \\\n    -not -path \"*/output/*\" \\\n    -not -path \"*/collider_output/*\" \\\n    -not -path \"*/.data/*\" \\\n    -not -path \"*/.collider/*\" \\\n    -print0 | xargs -0 zip -r full_contextome.zip\n```\n\n### What this includes (The \"Wave\" & \"Observer\" Realms)\n1.  **Documentation**: All `.md` files from `context-management/docs/`, `standard-model-of-code/docs/`, and `.agent/`.\n2.  **Configuration**: All `.yaml` files (including `aci_config.yaml`, `semantic_models.yaml`, `analysis_sets.yaml`).\n3.  **Schemas**: All `.json` schemas from `standard-model-of-code/schema/`.\n4.  **Agent State**: All task registries and sprint logs in `.agent/`.\n\n### What this excludes (To save space)\n1.  **Analysis Artifacts**: `unified_analysis.json` (often ~800MB).\n2.  **Virtual Envs**: `.tools_venv`, `node_modules`.\n3.  **Build Outputs**: `collider_output/`, `.collider/`.\n\n### Verification\nTo verify the contents before zipping, you can run a dry-list:\n\n```bash\nfind . -type f \\( -name \"*.md\" -o -name \"*.yaml\" -o -name \"*.json\" \\) \\\n    -not -path \"*/.git/*\" -not -path \"*/.venv/*\" -not -path \"*/.tools_venv/*\" \\\n    -not -path \"*/node_modules/*\" -not -path \"*/output/*\" -not -path \"*/collider_output/*\" \\\n    -not -path \"*/.data/*\" -not -path \"*/.collider/*\" | sort\n```\n\n### Why not use `archive.py`?\nThe tool `context-management/tools/archive/archive.py` (referenced in `ASSET_INVENTORY.md`) is designed to mirror the repository to Google Cloud Storage (GCS) for AI retrieval. While it handles the \"Brain\" logic, it doesn't currently support a local zip export of the full Contextome without also trying to push to the cloud.",
    "mode": "standard"
  }
}