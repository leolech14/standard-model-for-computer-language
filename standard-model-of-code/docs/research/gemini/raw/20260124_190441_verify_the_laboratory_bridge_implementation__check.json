{
  "_meta": {
    "saved_at": "2026-01-24T19:04:41.002759",
    "source": "gemini",
    "model": "gemini-3-pro-preview",
    "query": "Verify the Laboratory Bridge implementation. Check laboratory.py and laboratory_bridge.py. Does this solve the orphaned Scientist problem? What docs need updating?",
    "query_length": 163,
    "checksum": "sha256:9d0bcc3ddda17653a2b193f7d934a3ee7a9453b002ff9a28d8e3b96f23d4c7bf"
  },
  "response": {
    "content": "Based on the provided codebase context, I have analyzed `tools/research/laboratory.py`. Note that **`laboratory_bridge.py` was not included in the file context**, so my analysis infers its role as the client-side (Agent) consumer of `laboratory.py`.\n\n### 1. Verification of `laboratory.py` (The Scientist Facade)\n\nThe implementation of `tools/research/laboratory.py` serves as a robust **Anti-Corruption Layer** between the Agent (Wave) and the Research Scripts (Particle).\n\n**Key Strengths:**\n*   **Command Templating:** By using Jinja-like placeholders (`{python}`, `{repo_path}`, `{unified_analysis}`), it decouples the *intent* (\"run collider\") from the *implementation* (\"python collider.py ...\"). This allows researchers to refactor scripts without breaking the Agent, provided the template is updated.\n*   **Standardized Output (`ExperimentResult`):** The Agent no longer needs to parse heterogeneous CLI stdout. It receives a strictly typed JSON object containing status, artifact paths, and summaries.\n*   **Artifact Abstraction:** It manages `out_dir`, logging, and artifact persistence (`.laboratory_runs/`), keeping the repo clean and giving the Agent a reliable file system location to read results.\n*   **Robustness:** Includes timeouts, error aggregation, and fallback logic for hypothesis confirmation (`_infer_confirmed`).\n\n**Potential Weakness identified in `laboratory.py`:**\n*   **Implicit Dependencies:** It assumes the existence of specific internal artifacts (like `unified_analysis.json`) for the happy path. If the underlying research tool changes its output filename format, `laboratory.py` logic (specifically lines 220-226) will need manual updates.\n\n### 2. The Missing `laboratory_bridge.py`\n\nSince `laboratory_bridge.py` is missing from the context, I cannot verify the client implementation. However, based on `laboratory.py`, a correct bridge should:\n1.  **Wrap the CLI:** constructing `ExperimentRequest` objects.\n2.  **Inject Templates:** Providing default templates (e.g., `LAB_COLLIDER_CMD_TEMPLATE`) via environment variables or configuration, so the Agent code remains agnostic to the specific shell commands.\n3.  **Type Safety:** Convert the raw JSON output from `laboratory.py` back into Pydantic models for the Agent to use in reasoning loops.\n\n### 3. Does this solve the \"Orphaned Scientist\" Problem?\n\n**YES.**\n\nThe \"Orphaned Scientist\" problem occurs when research tooling evolves rapidly (changing arguments, output formats, script names), causing the Agent's hardcoded tool invocations to break, effectively \"orphaning\" the scientist's latest work from the Agent's capabilities.\n\n`laboratory.py` solves this by:\n\n1.  **Inversion of Control:** The Agent defines *what* it needs (a hypothesis test), but the configuration (templates) defines *how* to execute it.\n2.  **Stable Contract:** The `ExperimentResult` JSON schema is the stable contract. As long as `laboratory.py` is maintained to output this JSON, the underlying scripts (`run_corpus.py`, `evaluate_hypotheses.py`) can change completely.\n3.  **Dual Mode:** It supports both \"generate analysis\" (run collider) and \"analyze existing\" (pass `unified_analysis_path`), allowing the Agent to switch between generating expensive data and analyzing cached data without changing its logic.\n\n### 4. Documentation Updates Required\n\nTo fully operationalize this bridge, the following documentation updates are necessary:\n\n1.  **`docs/research/README.md`**:\n    *   **Add:** A section on \"The Laboratory Interface.\"\n    *   **Explain:** How to run experiments via `laboratory.py` locally (for researchers to verify their scripts work with the Agent).\n    *   **Define:** The standard command templates. Example:\n        ```bash\n        # Standard Collider Template\n        export LAB_COLLIDER_CMD_TEMPLATE='[\"{python}\", \"tools/research/run_corpus.py\", \"{repo_path}\", \"--output\", \"{out_dir}\"]'\n        ```\n\n2.  **`docs/architecture/WAVE_PARTICLE_BRIDGE.md` (New or Update)**:\n    *   Formalize the `ExperimentRequest` -> `laboratory.py` -> `ExperimentResult` data flow.\n    *   Explicitly state that Agents **must not** call `run_corpus.py` or `atom_coverage.py` directly anymore.\n\n3.  **`tools/research/README.md`**:\n    *   Update usage examples to favor `laboratory.py` as the entry point.\n\n### 5. Recommendations\n\n**A. Fix Hardcoded Filenames in `laboratory.py`**\nLines 220-226 hardcode `unified_analysis.json`.\n*   *Recommendation:* Make the output filename configurable in the request or template context, defaulting to `unified_analysis.json`.\n\n**B. Add `laboratory_bridge.py` Stub**\nIf `laboratory_bridge.py` does not exist, create it as a Python module that exposes a function `run_agent_experiment(...)` which wraps the `subprocess.run` call to `laboratory.py`.\n\n**C. Verification Script**\nCreate a `tools/research/verify_lab.py` that runs a mock experiment using `laboratory.py` against a known repo (like one of the pilot repos) to ensure the full pipeline (Command -> Lab -> Script -> Output -> Lab -> JSON) works.\n\n**Implementation of Missing `laboratory_bridge.py` (Draft):**\n\n```python\n# tools/research/laboratory_bridge.py\nimport subprocess\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom .laboratory import ExperimentRequest, ExperimentResult\n\nLAB_SCRIPT = Path(__file__).parent / \"laboratory.py\"\n\ndef run_experiment_sync(request: ExperimentRequest) -> ExperimentResult:\n    \"\"\"Synchronous bridge for Agents to call the Laboratory.\"\"\"\n    cmd = [sys.executable, str(LAB_SCRIPT), \"run\"]\n    \n    if request.repo_path:\n        cmd.extend([\"--repo\", str(request.repo_path)])\n    if request.unified_analysis_path:\n        cmd.extend([\"--unified-analysis\", str(request.unified_analysis_path)])\n    if request.hypothesis:\n        cmd.extend([\"--hypothesis\", request.hypothesis])\n    \n    # Pass templates via Env or Args (Args preferred for explicit control)\n    if request.collider_cmd_template:\n        cmd.extend([\"--collider-cmd-template\", request.collider_cmd_template])\n        \n    try:\n        # Capture stdout to get the artifact path\n        proc = subprocess.run(cmd, capture_output=True, text=True, check=False)\n        \n        # The last line of stdout is the artifact path\n        artifact_path = proc.stdout.strip().splitlines()[-1] if proc.stdout else \"\"\n        \n        if proc.returncode != 0 or not artifact_path.endswith(\".json\"):\n            return ExperimentResult(\n                run_id=request.run_id,\n                created_at_utc=\"\", \n                ok=False, \n                confirmed=None, \n                summary={}, \n                artifacts={}, \n                logs={\"stderr\": proc.stderr}, \n                errors=[\"Laboratory CLI failed\", proc.stderr]\n            )\n            \n        # Read the result file\n        with open(artifact_path) as f:\n            data = json.load(f)\n            return ExperimentResult(**data)\n            \n    except Exception as e:\n        return ExperimentResult(\n            run_id=request.run_id,\n            created_at_utc=\"\",\n            ok=False, \n            confirmed=None, \n            summary={}, \n            artifacts={}, \n            logs={}, \n            errors=[str(e)]\n        )\n```",
    "mode": "standard"
  }
}