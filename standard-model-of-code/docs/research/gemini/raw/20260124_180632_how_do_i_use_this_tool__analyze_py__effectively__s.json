{
  "_meta": {
    "saved_at": "2026-01-24T18:06:32.570616",
    "source": "gemini",
    "model": "gemini-3-pro-preview",
    "query": "How do I use this tool (analyze.py) effectively? Specifically:\n\n1. How do I select specific files for context using --file?\n2. What model options are available and how do I use Gemini 2.5 Pro with 1M context?\n3. What does the --tier option do? What's the difference between long_context and deep?\n4. How should I size context for a query about comparing two directories for duplication?\n5. What's the optimal context size for a 'duplication audit' query?\n\nGive me practical examples.",
    "query_length": 483,
    "checksum": "sha256:6e15eee9e499a9216a46b5e151c37f143f050d02a336d16222863b5b9303a872"
  },
  "response": {
    "content": "Based on the ACI (Adaptive Context Intelligence) codebase provided, here is the practical guide to using `analyze.py` effectively.\n\n### 1. Selecting specific files for context (`--file`)\n\nThe `analyze.py` tool uses the **Context Optimizer** (`context_optimizer.py`) to manage what gets sent to the LLM. While the tool often selects files automatically using **Refinery** (`refinery.py`), you can manually force files into the context.\n\n**How it works:**\nThe system typically \"sandwiches\" or \"front-loads\" critical files based on the `positioning` logic in `context_optimizer.py`.\n\n**Usage Examples:**\n\n*   **Single File:**\n    ```bash\n    python analyze.py \"Why is the pipeline failing?\" --file src/core/pipeline.py\n    ```\n*   **Multiple Files:**\n    ```bash\n    python analyze.py \"Check for circular imports\" --file src/core/pipeline.py src/utils/helpers.py\n    ```\n*   **Wildcards (Shell Expansion):**\n    ```bash\n    python analyze.py \"Explain the authentication flow\" --file src/auth/*.py\n    ```\n\n**Tip:** When you provide `--file`, the **Tier Router** likely treats this as a focused intent. If you provide many files, ensure you are using at least the `long_context` tier so the `refinery.py` logic doesn't aggressively truncate them to fit a small budget.\n\n---\n\n### 2. Model Options & Using Gemini (1M Context)\n\nAccording to `tier_router.py` and `aci_config.yaml`, models are generally mapped to **Tiers** rather than selected manually, though overrides exist.\n\n**Available Models (Internal mappings):**\n*   **Gemini 2.0 Flash (`gemini-2.0-flash-thinking-exp`):** Used by `flash_deep` tier. Supports up to **2M tokens**.\n*   **Gemini 3 Pro (`gemini-3-pro`):** Used by `long_context` tier. Supports **1M tokens**.\n*   **Gemini 2.0 Flash 001:** Used by `rag` tier (fast, lower latency).\n\n**How to use 1M+ Context:**\nYou do not need to specify the model version explicitly. Instead, you trigger the tier that utilizes that model.\n\n*   **For ~1M Context (Reasoning Focus):** Use `long_context`.\n    ```bash\n    python analyze.py \"Refactor the entire auth module\" --tier long_context\n    ```\n*   **For ~2M Context (Massive Analysis):** Use `flash_deep`.\n    ```bash\n    python analyze.py \"Analyze the entire codebase for architectural drift\" --tier flash_deep\n    ```\n\n---\n\n### 3. Understanding `--tier`: `long_context` vs `deep`\n\nThe **Tier Router** (`tier_router.py`) logic defines distinct use cases for these tiers.\n\n#### **`long_context` (Tier 2)**\n*   **Backend:** Defaults to `gemini-3-pro` or `gemini-2.0-flash-001`.\n*   **Budget:** ~200,000 to 1,000,000 tokens.\n*   **Use Case:** Multi-file reasoning, complex debugging, architecture review of specific modules.\n*   **Behavior:** It uses `refinery.py` to select relevant *sets* of files (e.g., \"pipeline\" set, \"theory\" set).\n*   **Command:**\n    ```bash\n    python analyze.py \"How do the semantic matcher and tier router interact?\" --tier long_context\n    ```\n\n#### **`deep` / `flash_deep` (Tier 4)**\n*   **Backend:** Defaults to `gemini-2.0-flash-thinking-exp`.\n*   **Budget:** Up to **2,000,000 tokens**.\n*   **Use Case:** \"Holistic\" questions, cross-repo comparisons, or queries containing words like \"comprehensive,\" \"everything,\" or \"all files.\"\n*   **Behavior:** It bypasses standard selection and loads the **RepoPack** (`repopack.py`)â€”a massive, deterministic snapshot of your entire file tree and critical code. It caches this snapshot using `cache_registry.py`.\n*   **Command:**\n    ```bash\n    python analyze.py \"Do we have duplication between the legacy tools and the new aci module?\" --tier deep\n    ```\n\n---\n\n### 4. Sizing context for comparing two directories\n\nComparing directories requires the LLM to \"see\" all files simultaneously to detect patterns. RAG (searching) will fail here because it only retrieves snippets.\n\n**Strategy:**\n1.  **Do not use RAG:** It will miss the duplication.\n2.  **Force `flash_deep`:** This ensures the context window is large enough (2M tokens) to hold both directories fully.\n3.  **Targeting:** If `analyze.py` supports directory flags, use them. Otherwise, rely on `flash_deep`'s behavior of loading the full `RepoPack`.\n\n**Example:**\n```bash\npython analyze.py \"Compare src/v1/utils and src/v2/common for duplicated logic\" --tier deep\n```\n\n**Why this works:**\nThe `tier_router.py` logic checks for keywords. If you don't use `--tier deep`, you can trigger it naturally by using \"trigger words\" defined in `FLASH_DEEP_TRIGGERS`:\n> *\"comprehensive\", \"holistic\", \"all files\", \"entire codebase\"*\n\n```bash\n# Implicitly triggers deep tier due to \"comprehensive\" keyword\npython analyze.py \"Perform a comprehensive duplication check between v1 and v2 utils\"\n```\n\n---\n\n### 5. Optimal Context Size for 'Duplication Audit'\n\nFor a duplication audit, you are fighting against \"Lost-in-the-Middle\" effects.\n\n*   **Recommended Config:** `tier: flash_deep`\n*   **Token Estimate:**\n    *   **Small Repos (<50 files):** ~50k - 100k tokens.\n    *   **Medium Repos:** ~200k - 500k tokens.\n    *   **Context Strategy:** The `repopack.py` tool formats the context into a single block. This is optimal for duplication audits because the model can attend to the entire tree structure in one pass.\n\n**Config Warning:**\nYour `aci_config.yaml` has a default hard cap:\n```yaml\ntoken_budgets:\n  hard_cap: 200000     # Absolute maximum\n```\n**However**, `tier_router.py` contains logic that overrides this for `FLASH_DEEP`:\n```python\nelif tier == Tier.FLASH_DEEP:\n    # FLASH_DEEP: Load EVERYTHING - 2M context capacity\n```\nSo, for a duplication audit, **always use `flash_deep`**. If you use `long_context`, you might hit the 200k safety cap defined in `context_optimizer.py`, which would truncate files and make the duplication audit impossible.\n\n**Summary Checklist for Duplication Audit:**\n1.  **Tier:** `deep` (or `flash_deep`)\n2.  **Keywords:** Use \"comprehensive\" or \"all files\" in your query.\n3.  **Expectation:** It will take longer (uses context caching), but it ensures the model sees 100% of the code.",
    "mode": "standard"
  }
}