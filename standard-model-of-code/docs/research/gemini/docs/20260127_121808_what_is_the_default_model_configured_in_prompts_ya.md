# Research: What is the default model configured in prompts.yaml?

> **Date:** 2026-01-27 12:18:08
> **Source:** gemini
> **Model:** gemini-2.5-flash
> **Checksum:** `sha256:fe1a3a65a50aa5850fb053d28c9ceacb2504c41144bb137ac29595424795e8cc`
> **Raw JSON:** `raw/20260127_121808_what_is_the_default_model_configured_in_prompts_ya.json`

---

## Query

What is the default model configured in prompts.yaml?

---

## Response

Based on the provided codebase context, there is **no mention** of a `prompts.yaml` file. Therefore, I cannot determine the default model configured within it.

The `.agent/manifest.yaml` lists configurations for AI tools like `gemini` (via `context-management/tools/ai/analyze.py`) and `perplexity` (via `context-management/tools/mcp/perplexity_mcp_server.py`), but these do not reference a `prompts.yaml` file.

---

## Citations

_No citations provided_
