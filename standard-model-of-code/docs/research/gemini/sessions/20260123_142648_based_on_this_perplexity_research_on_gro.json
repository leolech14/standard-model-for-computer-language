{
  "session_start": "2026-01-23 14:26:05",
  "session_end": "2026-01-23 14:26:48",
  "model": "gemini-3-pro-preview",
  "total_tokens_in": 34160,
  "total_tokens_out": 1669,
  "turn_count": 2,
  "turns": [
    {
      "timestamp": 1769189165.2596,
      "timestamp_str": "2026-01-23 14:26:05",
      "role": "user",
      "content": "Based on this Perplexity research on grounding external queries:\n\nPERPLEXITY RECOMMENDATIONS:\n1. Use HYBRID approach: Static template + LLM refinement\n2. Context prefix should be 300-600 tokens\n3. Structure: YAML/markdown block with markers (---context---)\n4. Include: project domain, key constraints, stack, 1-2 code examples\n5. Have Gemini extract query-relevant context before sending to Perplexity\n\nGiven our existing analyze.py architecture:\n1. Where exactly should the context injection code go?\n2. What existing functions/modules can we reuse?\n3. Should this be a new module or integrated into existing flow?\n4. What's the minimal change to implement this properly?\n\nProvide specific file paths and function names from the codebase."
    },
    {
      "timestamp": 1769189208.081171,
      "timestamp_str": "2026-01-23 14:26:48",
      "role": "assistant",
      "content": "Based on the analysis of your codebase and the Perplexity recommendations, here is the architectural plan and minimal implementation guide.\n\n### 1. Architecture: The Hybrid Grounding Flow\n\nInstead of sending the raw query directly to Perplexity, we will interject a \"Grounding Phase\" using the existing Gemini client in `analyze.py`.\n\n**Flow:**\n1.  **ACI Routing:** Detects `Tier.PERPLEXITY`.\n2.  **Context Loading:** Load `semantic_models.yaml` (Antimatter Laws/Domain) + `analysis_sets.yaml` (Stack definitions).\n3.  **Gemini Extraction:** Ask Gemini to \"Rewrite this query for an external researcher, appending minimal relevant context (Stack, Domain, Constraints) from our project.\"\n4.  **Perplexity Execution:** Send the *grounded* query to `perplexity_research.py`.\n\n### 2. Implementation Answers\n\n#### 1. Where exactly should the context injection code go?\nThe logic belongs in **`context-management/tools/ai/analyze.py`**. Specifically, inside the `main()` function within the `if decision.tier == Tier.PERPLEXITY:` block (approx line 1228).\n\n#### 2. What existing functions/modules can we reuse?\n*   **`load_semantic_models()`**: (Line 1076 in `analyze.py`) To fetch Domain definitions and Antimatter Laws.\n*   **`create_client()`**: (Line 923 in `analyze.py`) To initialize Gemini for the extraction step.\n*   **`semantic_models.yaml`**: As the source of \"Project Domain\" and \"Constraints\".\n\n#### 3. Should this be a new module?\n**No.** It should be a helper function within `analyze.py` or integrated directly into the `main` flow. Creating a new module for ~20 lines of \"glue code\" is over-engineering.\n\n#### 4. Minimal Change Implementation\nWe need to:\n1.  Hoist `create_client()` (or call it on demand) inside the Perplexity block.\n2.  Read `semantic_models.yaml`.\n3.  Run a quick \"One-Shot\" Gemini call to generate the grounded prompt.\n\n---\n\n### 3. Implementation Plan\n\nHere are the specific changes for **`context-management/tools/ai/analyze.py`**.\n\n#### A. Add Helper Function\nAdd this function near `load_semantic_models` (approx line 1082) to construct the context packet recommended by Perplexity (YAML/Markdown block).\n\n```python\ndef generate_grounded_perplexity_query(client, user_query, project_root):\n    \"\"\"\n    Uses Gemini to rewrite the user query with a 'Context Prefix' \n    based on semantic models (Stack, Domain, Constraints).\n    \"\"\"\n    # 1. Load Sources\n    semantic_config = load_semantic_models()\n    \n    # 2. Extract High-Level Context (Domain & Constraints)\n    # We map 'antimatter' laws to 'Constraints' and definitions to 'Domain'\n    domain_context = {\n        \"domain_definitions\": list(semantic_config.get('definitions', {}).keys()),\n        \"constraints\": [law['name'] for law in semantic_config.get('antimatter', [])],\n        \"tech_stack\": [\"Python 3\", \"Gemini API\", \"Vertex AI\", \"FastAPI\"] # Could be dynamic, but static is safer for now\n    }\n    \n    context_str = yaml.dump(domain_context, default_flow_style=False)\n\n    # 3. Prompt Gemini to fuse them (The Hybrid Approach)\n    prompt = f\"\"\"\n    You are a Research Proxy. Prepare a query for an external search engine (Perplexity).\n    \n    USER QUERY: \"{user_query}\"\n    \n    PROJECT CONTEXT (YAML):\n    {context_str}\n    \n    INSTRUCTIONS:\n    1. Rewrite the user query to be self-contained.\n    2. Append a Markdown block named \"---context---\" containing ONLY relevant stack details and constraints from the YAML above.\n    3. Keep the total length under 600 tokens.\n    4. Do NOT answer the question. Just output the refined query string.\n    \"\"\"\n    \n    try:\n        response = client.models.generate_content(\n            model=FAST_MODEL, # Use Flash for speed\n            contents=prompt\n        )\n        return response.text.strip()\n    except Exception as e:\n        print(f\"Warning: Grounding failed ({e}). Using raw query.\", file=sys.stderr)\n        return user_query\n```\n\n#### B. Modify the Perplexity Block\nUpdate the `if decision.tier == Tier.PERPLEXITY:` block in `main()` (approx line 1228).\n\n```python\n        # TIER 3: PERPLEXITY - External research\n        if decision.tier == Tier.PERPLEXITY:\n            print(\"[PERPLEXITY] External research tier selected.\", file=sys.stderr)\n            \n            # --- START NEW GROUNDING LOGIC ---\n            print(\"[PERPLEXITY] Grounding query with local context...\", file=sys.stderr)\n            \n            # 1. Initialize Gemini Client (needed for grounding step)\n            # Note: We ignore project_id here as we just need the client\n            grounding_client, _ = create_client()\n            \n            # 2. Generate Grounded Query\n            grounded_query = generate_grounded_perplexity_query(\n                grounding_client, \n                args.prompt, \n                PROJECT_ROOT\n            )\n            \n            if args.aci_debug:\n                print(f\"\\n[Grounding] Rewritten Query:\\n{grounded_query}\\n\", file=sys.stderr)\n            # --- END NEW GROUNDING LOGIC ---\n\n            if not HAS_PERPLEXITY:\n                print(\"Perplexity module not available. Falling back to LONG_CONTEXT.\", file=sys.stderr)\n                decision = analyze_and_route(args.prompt, force_tier=\"long_context\")\n            else:\n                start_time = time.time()\n                try:\n                    print(f\"[PERPLEXITY] Executing research query...\", file=sys.stderr)\n                    \n                    # CHANGED: Pass grounded_query instead of args.prompt\n                    result = perplexity_research(grounded_query) \n                    \n                    duration_ms = int((time.time() - start_time) * 1000)\n\n                    # Display results (Existing code follows...)\n                    print(\"\\n\" + \"=\" * 60)\n                    print(\"PERPLEXITY RESEARCH RESULTS\")\n                    # ... rest of existing block\n```\n\n### 4. Summary of Changes\n\n| File | Change | Purpose |\n| :--- | :--- | :--- |\n| `analyze.py` | Add `generate_grounded_perplexity_query` | Implements the \"Hybrid\" static template + LLM refinement logic. |\n| `analyze.py` | Update `Tier.PERPLEXITY` block | Initializes Gemini client early to rewrite the query before sending to Perplexity. |\n| `semantic_models.yaml` | (Read-only access) | Used as the single source of truth for the \"Context Prefix\". |\n\nThis implementation satisfies the Perplexity recommendation of sending a structured `---context---` block while leveraging your existing `semantic_models.yaml` as the source of truth, without requiring new files or heavy refactoring."
    }
  ]
}