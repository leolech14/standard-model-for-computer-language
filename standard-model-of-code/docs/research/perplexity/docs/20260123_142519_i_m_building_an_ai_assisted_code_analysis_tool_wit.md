# Research: I'm building an AI-assisted code analysis tool with a multi-tier query routing system:

- TIER 1 (RA...

> **Date:** 2026-01-23 14:25:19
> **Source:** perplexity-mcp
> **Model:** sonar-pro
> **Checksum:** `sha256:5ca4bb7a60e29c3859fa1a012a33fc7fd5d54cffb80644c9120b5d89e6cb1d17`
> **Raw JSON:** `raw/20260123_142519_i_m_building_an_ai_assisted_code_analysis_tool_wit.json`

---

## Query

I'm building an AI-assisted code analysis tool with a multi-tier query routing system:

- TIER 1 (RAG): Local file search with citations
- TIER 2 (LONG_CONTEXT): Gemini with full local codebase context (up to 1M tokens)
- TIER 3 (PERPLEXITY): External web research via Perplexity API

CURRENT PROBLEM:
When routing to Perplexity (Tier 3), the local codebase context is completely ignored. The raw user query goes directly to Perplexity. This means external research isn't grounded in our specific project context.

PROPOSED FIX:
Before sending to Perplexity, generate a summary of relevant local context and prefix it to the query.

QUESTIONS:

1. What is the best practice for 'grounding' external LLM/search queries with local context? Is prefixing a summary the right approach, or are there better patterns?

2. What should the context summary include for maximum relevance without exceeding Perplexity's input limits?
   - Just key terms/entities?
   - A structured brief (project name, domain, key concepts)?
   - Relevant code snippets?

3. Should the summary be generated by:
   - A) Static template filled with metadata
   - B) LLM-generated summary (Gemini summarizes context, then Perplexity researches)
   - C) Hybrid (template + LLM refinement)

4. Are there established patterns like 'Retrieval-Augmented Research' or 'Grounded Web Search' that address this?

5. What's the optimal summary length for Perplexity's sonar-pro model to maintain relevance without noise?

Provide concrete recommendations with rationale.

---

## Response

# Grounding External Search with Local Context

Your proposed fix aligns with established best practices. **Prefixing a context summary before external search queries is a valid and widely-used grounding pattern**, though the specific content and generation method should be carefully considered for your use case.

## Best Practices for Grounding External Queries

**Context prefixing is the right foundational approach.** The principle of grounding—linking abstract knowledge to real-world specifics—applies equally to external searches as to local analysis[1]. When you prefix local context to a Perplexity query, you're implementing what amounts to **prompt grounding with external knowledge bases**, similar to how systems augment LLM prompts with semantic search results[1].

However, research shows that effectiveness depends heavily on *what* context you include and *how* it's structured. Raw context injection has diminishing returns; systems that selectively extract task-relevant information significantly outperform those that include comprehensive but unfocused context[2]. The goal is to avoid "context pollution"—providing so much background that the external system struggles to identify what's actually relevant[6].

## Optimal Context Summary Composition

For maximum relevance without noise, your summary should follow a **structured brief format with selective code references**:

| Element | Inclusion | Rationale |
|---------|-----------|-----------|
| Project domain/purpose | Always | Helps Perplexity interpret the use case correctly[1] |
| Key architectural concepts | Always | Frames the problem within your system's paradigm[5] |
| Relevant technical constraints | Always | Prevents generic solutions misaligned with your stack[2] |
| Specific code snippets | Selectively | Include only 1-3 representative examples (50-100 lines total) when they directly illustrate the problem[1] |
| Full API/function signatures | Yes | Critical for getting accurate external recommendations[1] |
| Metadata (framework versions, dependencies) | Conditionally | Only if directly relevant to the query[2] |

**Avoid:** Entire file listings, extensive architectural documentation, or historical context unrelated to the specific question.

**Structure it as:** A brief YAML or markdown block at the query prefix containing: project context, domain, key constraints, and 1-2 focused code examples[2].

Example format:
```
---context---
PROJECT: Analytics Pipeline (Python/PySpark)
DOMAIN: Real-time data processing for financial markets
KEY_CONSTRAINT: Sub-100ms latency requirement
STACK: PySpark 3.x, Delta Lake, Kafka

RELEVANT_CODE:
[1-2 representative snippets showing the actual problem area]
---/context---

[User's actual query to Perplexity]
```

## Recommended Generation Method: Hybrid Approach (Option C)

**Use a hybrid strategy: LLM-refined template.** Here's why:

1. **Start with a static template** populated from your codebase metadata (framework, key modules, constraints)[1]. This is fast and prevents hallucination.

2. **Have Gemini refine it** specifically for the current query. Gemini should:
   - Extract only the context relevant to the user's question (not all project context)[2]
   - Identify the 1-2 most illustrative code snippets if applicable
   - Compress domain-specific terminology into Perplexity-friendly explanations

3. **Why not pure LLM generation?** LLMs can hallucinate context details when generating from scratch. A template keeps the foundation grounded in actual project metadata[2][4].

This hybrid approach balances **reduced hallucinations** (via templating) with **task-specific relevance** (via LLM refinement)[2].

## Established Patterns

The pattern you're describing already exists in literature, though it's not universally standardized by a single name:

- **Retrieval-Augmented Generation (RAG)** with external sources[1]: Your Tier 3 + context summary is essentially RAG applied to web search.
- **Context Engineering** (Anthropic's framing)[6]: Giving AI systems "the right information at the right time"—directly applicable to your multi-tier system.
- **Sensorimotor + Symbolic Grounding** (academic framing)[4]: Connecting abstract queries to concrete local system knowledge.

Atlassian's approach with Rovo Dev and similar developer tools use this exact pattern: local codebase grounding before external reasoning[9].

## Optimal Summary Length for Perplexity Sonar-Pro

**Target 300-600 tokens for the context prefix; keep the actual query concise.**

Rationale:
- Sonar-Pro has effectively unlimited context, so token limits aren't the constraint[6]
- But **relevance degrades with verbose prefixes**—models show reduced precision for information retrieval when context is diluted[6]
- A tight, structured 300-600 token summary maintains signal-to-noise ratio
- This leaves ample room for Perplexity's web search reasoning without competing information

**If the context summary would exceed 800 tokens, you're likely over-sharing.** Re-evaluate whether each piece is truly necessary for the query.

## Concrete Implementation Recommendation

1. **Create a context extractor** that generates the hybrid summary:
   - Template layer: project metadata + relevant constraints (static, fast)
   - Gemini refinement: filter template for query relevance + select code examples (10-20s latency)

2. **Prefix the summary** to every Perplexity query from Tier 3

3. **Include a marker** (like `---context---`) to help Perplexity distinguish context from the actual question

4. **A/B test** two variants:
   - Minimal context (just domain + key constraints)
   - Rich context (+ code examples)

Measure whether external research recommendations are actually more aligned with your codebase specifics with the context included.

This hybrid, structured approach directly addresses the "context pollution" problem that reduces AI agent effectiveness across extended interactions[6], while implementing the grounding principle at each tier of your pipeline.

---

## Citations

1. https://www.moveworks.com/us/en/resources/blog/what-is-grounding-ai
2. https://github.com/pgalko/BambooAI/wiki/Grounding-Data-Analysis-with-Domain%E2%80%90Specific-Ontologies
3. https://sourcegraph.com/blog/lessons-from-building-ai-coding-assistants-context-retrieval-and-evaluation
4. https://dev.to/get_pieces/how-context-grounding-ai-decreases-hallucinations-3p8a
5. https://insights.daffodilsw.com/blog/vibe-coding-isnt-enough-how-to-build-context-aware-systems-with-ai
6. https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents
7. https://www.elastic.co/what-is/context-engineering
8. https://www.salesforce.com/blog/what-is-grounding/
9. https://redmonk.com/videos/context-engineering-in-practice-how-atlassian-is-building-ai-for-real-developer-work-with-kun-chen/

---

## Usage Stats

- Prompt tokens: 343
- Completion tokens: 1213
- Total tokens: 1556
