# Research Schemas Configuration
# ================================
# TARGET AUDIENCE: AI AGENTS
#
# This file defines reusable research patterns that orchestrate
# multiple ACI queries with different configurations to produce
# validated, multi-perspective answers.
#
# AI AGENT INSTRUCTIONS:
# ----------------------
# 1. Use --research <schema_name> to execute a predefined schema
# 2. Use --research-custom <json> for ad-hoc configurations
# 3. Use --list-research-schemas to discover available schemas
# 4. Use --describe-schema <name> to see schema details
#
# JOYSTICK PARAMETERS (overridable per-run):
# ------------------------------------------
# - model: Which LLM to use
# - tier: ACI execution tier (see TIER CATALOG below)
# - sets: Which analysis_sets to load
# - token_budget: Maximum tokens for context
# - temperature: Model creativity (0.0-1.0)
# - system_prompt: Override default behavior
#
# TIER CATALOG (validated on load):
# ---------------------------------
# - instant: Cached truths (<100ms, $0)
# - rag: File Search with citations (~5s, $0.01)
# - long_context: Full Gemini reasoning (~60s, $0.10)
# - perplexity: External web research (~30s, $0.05)
# - flash_deep: 2M context window (~90s, $0.20)
# - hybrid: Internal + external combined (~120s, $0.15)
#
# RUN TYPES:
# ----------
# - internal: Uses repo context (sets, token_budget allowed)
# - external: Web/API only (sets/token_budget IGNORED, membrane enforced)
#
# SYNTHESIS STRATEGIES:
# --------------------
# - consensus: Find agreement across runs (majority voting)
# - quality_gradient: Compare answer quality at different depths
# - dialectic: Thesis + antithesis → synthesis
# - triangulation: Cross-reference multiple angles
# - bayesian: Update confidence based on evidence types
# - hierarchical: Multi-scale aggregation

# =============================================================================
# GLOBAL DEFAULTS
# =============================================================================
# All runs inherit these unless explicitly overridden.

defaults:
  model: "gemini-3-pro-preview"
  tier: "long_context"
  temperature: 0.2
  token_budget: 100000
  type: "internal"  # internal | external

# =============================================================================
# MODEL ALIASES (runtime validation)
# =============================================================================
# Canonical model IDs. Schema loader MUST validate against this list.

model_catalog:
  - "gemini-3-pro-preview"      # Primary reasoning model
  - "gemini-2.5-pro"            # Stable fallback
  - "gemini-2.0-flash-001"      # Fast, cost-effective
  - "sonar-pro"                 # Perplexity default

# =============================================================================
# SCHEMA DEFINITIONS
# =============================================================================

research_schemas:

  # ---------------------------------------------------------------------------
  # VALIDATION_TRIO: Cross-model verification for high-stakes claims
  # ---------------------------------------------------------------------------
  # USE WHEN: You need high confidence in an answer
  # COST: ~$0.50 (3 API calls)
  # TIME: ~90s (parallel) or ~180s (sequential)
  # ---------------------------------------------------------------------------
  validation_trio:
    description: "Cross-model verification: Run same query through 3 configurations"
    purpose: "Detect hallucinations and build consensus"
    recommended_for:
      - "Architectural claims"
      - "Count/inventory questions"
      - "Existence checks (does X exist?)"

    runs:
      - name: "reasoning"
        description: "Deep reasoning with full context"
        type: "internal"
        model: "gemini-3-pro-preview"
        tier: "long_context"
        sets: ["theory", "pipeline"]
        token_budget: 150000
        temperature: 0.2

      - name: "fast"
        description: "Quick validation with focused context"
        type: "internal"
        model: "gemini-2.0-flash-001"
        tier: "long_context"
        sets: ["theory"]
        token_budget: 50000
        temperature: 0.1

      - name: "external"
        description: "External grounding (if applicable)"
        type: "external"
        tier: "perplexity"
        # DSL condition: scope must be EXTERNAL or HYBRID
        condition:
          scope_in: ["EXTERNAL", "HYBRID"]
        # SKIP if condition fails (do NOT duplicate reasoning)
        fallback: "skip"

    synthesis:
      strategy: "consensus"
      min_agreement: 2  # At least 2/3 must agree
      # CRITICAL: Require distinct sources to prevent artificial agreement
      distinct_sources_required: true
      output_format: "structured"
      fields:
        - consensus_answer
        - agreement_score
        - disagreements
        - citations_by_run
        - sources_used  # Which runs actually contributed

  # ---------------------------------------------------------------------------
  # DEPTH_LADDER: Find optimal context size for query type
  # ---------------------------------------------------------------------------
  # USE WHEN: Calibrating token budgets or testing context sensitivity
  # COST: ~$0.80 (4 API calls at increasing sizes)
  # TIME: ~120s
  # ---------------------------------------------------------------------------
  depth_ladder:
    description: "Test answer quality at increasing context depths"
    purpose: "Find minimum viable context for a query class"
    recommended_for:
      - "Query type calibration"
      - "Cost optimization research"
      - "Context sensitivity analysis"

    runs:
      - name: "minimal"
        description: "Minimal context - test baseline"
        type: "internal"
        sets: ["theory"]
        token_budget: 10000
        label: "10K"

      - name: "focused"
        description: "Focused context - single domain"
        type: "internal"
        sets: ["pipeline"]
        token_budget: 50000
        label: "50K"

      - name: "balanced"
        description: "Balanced context - multiple domains"
        type: "internal"
        sets: ["pipeline", "theory", "classifiers"]
        token_budget: 150000
        label: "150K"

      - name: "comprehensive"
        description: "Maximum context - full analysis"
        type: "internal"
        tier: "flash_deep"
        sets: ["architecture_review", "agent_full", "theory"]
        token_budget: 500000
        label: "500K"

    synthesis:
      strategy: "quality_gradient"
      # DETERMINISTIC metrics only (no LLM self-judgment)
      metrics:
        deterministic:
          - citation_count: "Count of file:line references"
          - output_length: "Response token count"
          - set_coverage: "Fraction of input sets mentioned"
          - code_block_count: "Number of code snippets"
        # Qualitative metrics require separate judge run
        qualitative_judge:
          enabled: false  # Set true to add judge run
          model: "gemini-2.0-flash-001"
          criteria:
            - completeness
            - specificity
            - confidence
      output_format: "ladder_report"

  # ---------------------------------------------------------------------------
  # ADVERSARIAL_PAIR: Thesis vs Antithesis → Synthesis
  # ---------------------------------------------------------------------------
  # USE WHEN: Testing robustness of a claim or hypothesis
  # COST: ~$0.40 (2 API calls + synthesis)
  # TIME: ~90s
  # ---------------------------------------------------------------------------
  adversarial_pair:
    description: "Generate opposing perspectives, then reconcile"
    purpose: "Stress-test claims and find edge cases"
    recommended_for:
      - "Hypothesis validation"
      - "Design decision evaluation"
      - "Risk assessment"

    runs:
      - name: "advocate"
        description: "Find supporting evidence"
        type: "internal"
        system_prompt: |
          You are an ADVOCATE. Your task is to find ALL evidence that SUPPORTS
          the following claim. Be thorough and cite specific code locations.
          Do not mention counterarguments.
        sets: ["theory", "pipeline", "docs_core"]
        temperature: 0.3

      - name: "skeptic"
        description: "Find opposing evidence"
        type: "internal"
        system_prompt: |
          You are a SKEPTIC. Your task is to find ALL evidence that CONTRADICTS
          or WEAKENS the following claim. Look for edge cases, exceptions, and
          gaps. Do not mention supporting evidence.
        sets: ["theory", "pipeline", "docs_core"]
        temperature: 0.3

    synthesis:
      strategy: "dialectic"
      output_format: "structured"
      fields:
        - thesis: "Supporting arguments"
        - antithesis: "Opposing arguments"
        - synthesis: "Reconciled position"
        - robustness_score: "0-100"
        - vulnerabilities: "Identified weak points"

  # ---------------------------------------------------------------------------
  # FORENSIC_INVESTIGATION: Multi-angle bug/issue analysis
  # ---------------------------------------------------------------------------
  # USE WHEN: Debugging complex issues
  # COST: ~$0.60 (4 specialized runs)
  # TIME: ~150s
  # ---------------------------------------------------------------------------
  forensic_investigation:
    description: "Investigate issue from multiple angles"
    purpose: "Root cause analysis through triangulation"
    recommended_for:
      - "Bug investigation"
      - "Test failure analysis"
      - "Performance regression hunting"

    runs:
      - name: "structural"
        description: "Analyze code structure"
        type: "internal"
        system_prompt: |
          Analyze the CODE STRUCTURE that could cause this issue.
          Focus on: function signatures, class hierarchies, data flow.
          Cite specific files and line numbers.
        sets: ["pipeline", "classifiers"]

      - name: "behavioral"
        description: "Analyze runtime behavior"
        type: "internal"
        system_prompt: |
          Analyze the RUNTIME BEHAVIOR that could cause this issue.
          Focus on: control flow, state mutations, error paths.
          Cite specific functions and their interactions.
        sets: ["pipeline", "tests"]

      - name: "dependency"
        description: "Analyze dependencies"
        type: "internal"
        system_prompt: |
          Trace DEPENDENCIES that could be involved in this issue.
          Focus on: imports, external calls, configuration.
          Cite specific dependency chains.
        sets: ["schema", "constraints"]

      - name: "historical"
        description: "Check for recent changes"
        type: "internal"
        system_prompt: |
          Based on the code patterns, identify what CHANGES could have
          introduced this issue. Look for: incomplete refactors,
          missing migrations, version mismatches.
        sets: ["docs_core", "research_core"]

    synthesis:
      strategy: "triangulation"
      output_format: "investigation_report"
      fields:
        - primary_hypothesis
        - supporting_evidence
        - alternative_hypotheses
        - recommended_actions
        - confidence_level

  # ---------------------------------------------------------------------------
  # CONFIDENCE_CALIBRATION: Bayesian evidence evaluation
  # ---------------------------------------------------------------------------
  # USE WHEN: Need precise confidence score for a claim
  # COST: ~$0.50 (4 evidence-gathering runs)
  # TIME: ~120s
  # ---------------------------------------------------------------------------
  confidence_calibration:
    description: "Calibrate confidence through evidence classification"
    purpose: "Produce justified confidence scores"
    recommended_for:
      - "Claim verification"
      - "Documentation accuracy"
      - "Architecture compliance"

    runs:
      - name: "direct_evidence"
        description: "Find direct proof"
        type: "internal"
        system_prompt: |
          Find DIRECT evidence for this claim. Direct evidence means:
          - Explicit code that implements the claim
          - Documentation that states the claim
          - Tests that verify the claim
          Provide file:line citations for each piece of evidence.
        citation_required: true
        sets: ["pipeline", "theory", "tests"]

      - name: "indirect_evidence"
        description: "Find circumstantial support"
        type: "internal"
        system_prompt: |
          Find INDIRECT evidence for this claim. Indirect evidence means:
          - Related code that implies the claim
          - Patterns consistent with the claim
          - Absence of contradicting code
          Explain the reasoning chain for each piece.
        sets: ["pipeline", "schema"]

      - name: "counter_evidence"
        description: "Find contradictions"
        type: "internal"
        system_prompt: |
          Find COUNTER-EVIDENCE against this claim. Look for:
          - Code that contradicts the claim
          - Documentation inconsistencies
          - Test failures or gaps
          Be thorough - even weak counter-evidence matters.
        sets: ["pipeline", "theory", "tests"]

      - name: "external_check"
        description: "External validation"
        type: "external"
        tier: "perplexity"
        # DSL condition: run only if scope is not purely internal
        condition:
          scope_not: "INTERNAL"
        fallback: "skip"

    synthesis:
      strategy: "bayesian"
      prior: 0.5  # Start with maximum uncertainty
      weights:
        direct_evidence: 0.4
        indirect_evidence: 0.2
        counter_evidence: -0.3
        external_check: 0.1
      output_format: "confidence_scorecard"

  # ---------------------------------------------------------------------------
  # SEMANTIC_PROBE: Test understanding at different abstraction levels
  # ---------------------------------------------------------------------------
  # USE WHEN: Exploring a concept across the scale hierarchy
  # COST: ~$0.40 (3 runs at different levels)
  # TIME: ~90s
  # ---------------------------------------------------------------------------
  semantic_probe:
    description: "Probe concept at different abstraction levels"
    purpose: "Understand how a concept manifests across scales"
    recommended_for:
      - "Concept exploration"
      - "Architecture understanding"
      - "Scale-level analysis"

    runs:
      - name: "node_level"
        description: "L3 NODE: Function/method level"
        type: "internal"
        system_prompt: |
          Analyze this at the NODE level (L3 - functions/methods).
          What specific functions implement this concept?
          Cite function names and signatures.
        sets: ["pipeline", "classifiers"]

      - name: "module_level"
        description: "L5 MODULE: File level"
        type: "internal"
        system_prompt: |
          Analyze this at the MODULE level (L5 - files).
          Which files are responsible for this concept?
          How do they organize the functionality?
        sets: ["pipeline", "docs_core"]

      - name: "system_level"
        description: "L7 SYSTEM: Subsystem level"
        type: "internal"
        system_prompt: |
          Analyze this at the SYSTEM level (L7 - subsystems).
          Which subsystems interact around this concept?
          What are the integration points?
        sets: ["agent_kernel", "theory"]

    synthesis:
      strategy: "hierarchical"
      output_format: "scale_map"
      fields:
        - l3_nodes: "Functions involved"
        - l5_modules: "Files involved"
        - l7_systems: "Subsystems involved"
        - cross_level_flows: "How data/control flows across levels"

  # ---------------------------------------------------------------------------
  # CLAUDE_HISTORY_INGEST: Load Claude Code conversation history
  # ---------------------------------------------------------------------------
  # USE WHEN: Mining past conversations for insights, decisions, patterns
  # COST: ~$0.30 (1 large context load + synthesis)
  # TIME: ~120s (depends on history size)
  # ---------------------------------------------------------------------------
  claude_history_ingest:
    description: "Load Claude Code JSONL history into long context for mining"
    purpose: "Extract insights, decisions, and patterns from past conversations"
    recommended_for:
      - "Finding past architectural decisions"
      - "Recovering lost context from compacted sessions"
      - "Building knowledge graph from conversation history"
      - "Mining implementation patterns across sessions"

    # Source configuration (used by history loader)
    source:
      type: "claude_code_jsonl"
      location: "~/.claude/projects/-Users-lech-PROJECTS-all-PROJECT-elements/"
      # Selection modes
      selection:
        mode: "recent"  # recent | specific | all | date_range
        count: 5        # For "recent" mode: last N sessions
        # For "specific" mode: session_ids: ["abc123", "def456"]
        # For "date_range" mode: start_date, end_date

    # Pre-processing pipeline
    preprocessing:
      # Extract only meaningful content
      extract_types:
        - "user"        # User messages
        - "assistant"   # Assistant responses
        # Skip: file-history-snapshot, thinking (unless requested)
      include_thinking: false  # Include <thinking> blocks?
      max_tokens_per_session: 100000  # Truncate long sessions
      deduplication: true  # Remove duplicate content
      # Transform to structured format
      output_format: "conversation_turns"  # conversation_turns | raw_jsonl | summary

    runs:
      - name: "history_load"
        description: "Load conversation history into 1M+ context"
        type: "internal"
        tier: "flash_deep"  # 2M context window
        model: "gemini-2.0-flash-001"  # Fast, large context
        token_budget: 1000000
        temperature: 0.1
        system_prompt: |
          You are analyzing Claude Code conversation history from PROJECT_elements.
          The history contains past AI-human collaboration sessions about:
          - Standard Model of Code (Collider, atoms, roles)
          - Wave-Particle architecture
          - Purpose Field theory
          - ACI system design

          Extract and organize:
          1. KEY DECISIONS: Architectural choices made
          2. INSIGHTS: Theoretical discoveries
          3. PATTERNS: Recurring implementation approaches
          4. OPEN QUESTIONS: Unresolved issues mentioned
          5. CODE CHANGES: Significant modifications referenced

          Cite session IDs and approximate positions when referencing.

      - name: "cross_reference"
        description: "Validate history insights against current codebase"
        type: "internal"
        tier: "long_context"
        model: "gemini-3-pro-preview"
        sets: ["theory", "pipeline", "docs_core"]
        token_budget: 150000
        temperature: 0.2
        system_prompt: |
          You are cross-referencing insights from past conversation history
          against the CURRENT state of the codebase.

          For each insight or decision mentioned:
          1. VERIFY: Is it still accurate?
          2. TRACE: Where is it implemented now?
          3. DRIFT: Has implementation diverged from discussion?

          Cite specific files and line numbers.

    synthesis:
      strategy: "triangulation"
      output_format: "structured"
      fields:
        - decisions_extracted: "Key architectural decisions with session source"
        - insights_mined: "Theoretical insights discovered"
        - patterns_identified: "Recurring implementation patterns"
        - drift_detected: "Where reality differs from discussion"
        - knowledge_graph: "Entity-relationship map of concepts discussed"
        - open_questions: "Unresolved issues from history"

  # ---------------------------------------------------------------------------
  # MIND_MAP_BUILDER: Build knowledge graph from history
  # ---------------------------------------------------------------------------
  # USE WHEN: Creating visual/structural representation of knowledge
  # COST: ~$0.50 (multi-pass extraction)
  # TIME: ~180s
  # ---------------------------------------------------------------------------
  mind_map_builder:
    description: "Build knowledge graph/mind map from conversation history"
    purpose: "Create structured knowledge representation for RAG indexing"
    recommended_for:
      - "Building searchable knowledge base"
      - "Creating concept maps"
      - "Identifying knowledge gaps"
      - "Preparing RAG corpus"

    source:
      type: "claude_code_jsonl"
      location: "~/.claude/projects/-Users-lech-PROJECTS-all-PROJECT-elements/"
      selection:
        mode: "all"  # Process entire history

    preprocessing:
      extract_types: ["user", "assistant"]
      include_thinking: true  # Include reasoning for deeper extraction
      deduplication: true
      output_format: "conversation_turns"

    runs:
      - name: "entity_extraction"
        description: "Extract named entities and concepts"
        type: "internal"
        tier: "flash_deep"
        model: "gemini-2.0-flash-001"
        token_budget: 1000000
        temperature: 0.0  # Deterministic
        system_prompt: |
          Extract ALL named entities from this conversation history.

          Categories:
          - CONCEPTS: Theoretical terms (Purpose Field, Wave-Particle, etc.)
          - FILES: Code files mentioned (path/to/file.py)
          - FUNCTIONS: Specific functions/methods
          - SCHEMAS: Data structures/formats
          - TOOLS: Commands/utilities
          - PEOPLE: Contributors (if mentioned)

          Output as JSON array:
          [{"entity": "...", "type": "...", "context": "...", "frequency": N}]

      - name: "relationship_extraction"
        description: "Extract relationships between entities"
        type: "internal"
        tier: "flash_deep"
        model: "gemini-2.0-flash-001"
        token_budget: 1000000
        temperature: 0.0
        system_prompt: |
          Given the entities extracted, identify RELATIONSHIPS between them.

          Relationship types:
          - implements: X implements Y
          - depends_on: X depends on Y
          - contains: X contains Y
          - contradicts: X contradicts Y
          - evolved_from: X evolved from Y
          - related_to: X is related to Y

          Output as JSON array:
          [{"source": "...", "target": "...", "type": "...", "evidence": "..."}]

      - name: "cluster_analysis"
        description: "Identify topic clusters"
        type: "internal"
        tier: "long_context"
        model: "gemini-3-pro-preview"
        token_budget: 200000
        temperature: 0.2
        system_prompt: |
          Analyze the extracted entities and relationships to identify
          TOPIC CLUSTERS - groups of related concepts that form coherent
          knowledge domains.

          For each cluster:
          1. NAME: Descriptive cluster name
          2. CORE_CONCEPTS: Central entities
          3. PERIPHERY: Related but less central
          4. GAPS: Missing concepts that should exist
          5. MATURITY: How well-developed is this area?

    synthesis:
      strategy: "hierarchical"
      output_format: "structured"
      fields:
        - entities: "All extracted entities with metadata"
        - relationships: "Entity relationship map"
        - clusters: "Topic clusters with maturity scores"
        - gaps: "Identified knowledge gaps"
        - rag_ready_chunks: "Chunks prepared for RAG indexing"
        - mermaid_diagram: "Mermaid.js visualization code"

  # ---------------------------------------------------------------------------
  # THEORETICAL_DISCUSSION: Deep framework analysis with expert critique
  # ---------------------------------------------------------------------------
  # USE WHEN: Developing or validating theoretical frameworks
  # COST: ~$1.00 (4 runs: 2 internal + 2 external)
  # TIME: ~180s
  # ---------------------------------------------------------------------------
  theoretical_discussion:
    description: "Deep discussion of theoretical frameworks with multi-perspective critique"
    purpose: "Develop, validate, and extend theoretical models through expert-level analysis"
    recommended_for:
      - "Framework development (like Communication Fabric)"
      - "Control theory model validation"
      - "Architectural hypothesis testing"
      - "Novel concept exploration"

    runs:
      - name: "internal_analysis"
        description: "Analyze framework against existing codebase architecture"
        type: "internal"
        model: "gemini-2.5-pro"
        tier: "long_context"
        sets: ["theory", "agent_kernel", "pipeline"]
        token_budget: 200000
        temperature: 0.3
        system_prompt: |
          You are a systems architect reviewing a theoretical framework.
          Analyze how this framework maps to the existing PROJECT_elements architecture.

          Evaluate:
          1. ALIGNMENT: How does this fit with Wave/Particle, Codome/Contextome?
          2. IMPLEMENTATION: What existing components support or conflict?
          3. GAPS: What's missing in current architecture to support this?
          4. CONCRETE EXAMPLES: Cite specific files/functions that relate.

          Be critical. Challenge assumptions. Cite file:line references.

      - name: "control_theory_review"
        description: "Evaluate from control systems perspective"
        type: "internal"
        model: "gemini-2.5-pro"
        tier: "long_context"
        sets: ["theory", "docs_core"]
        token_budget: 150000
        temperature: 0.2
        system_prompt: |
          You are a control systems engineer reviewing this framework.

          Evaluate:
          1. STATE VARIABLES: Are they observable? Controllable?
          2. STABILITY: Can the system diverge? What are attractors?
          3. FEEDBACK LOOPS: Are they properly damped? Any oscillation risk?
          4. TRANSFER FUNCTIONS: Can relationships be quantified?
          5. LYAPUNOV: Is there a candidate stability function?

          Apply rigorous control theory. Don't just describe - analyze dynamics.

      - name: "academic_grounding"
        description: "Ground in academic literature"
        type: "external"
        tier: "perplexity"
        system_prompt: |
          Search for peer-reviewed research validating or contradicting this framework.
          Find:
          1. Prior art in software engineering applying these concepts
          2. Empirical studies with measured outcomes
          3. Known limitations or failures of similar approaches
          4. Researchers/labs working on related problems

      - name: "novel_extension"
        description: "Propose extensions and novel contributions"
        type: "internal"
        model: "gemini-2.5-pro"
        tier: "long_context"
        sets: ["theory", "research_core"]
        token_budget: 150000
        temperature: 0.4
        system_prompt: |
          You are a research scientist identifying novel contributions.

          Identify:
          1. WHAT'S GENUINELY NEW: What hasn't been done before?
          2. WHAT'S PUBLISHABLE: What could be a conference paper?
          3. EXTENSIONS: How could this framework be extended?
          4. EXPERIMENTS: What would validate/falsify the claims?
          5. APPLICATIONS: Where else could this apply?

          Be ambitious but grounded. Cite connections to existing work.

    synthesis:
      strategy: "dialectic"
      output_format: "structured"
      fields:
        - framework_validation: "Does the framework hold up?"
        - control_analysis: "Stability, dynamics, feedback assessment"
        - prior_art: "Academic grounding and citations"
        - novel_contributions: "What's genuinely new"
        - gaps_identified: "What needs more work"
        - recommended_experiments: "How to validate"
        - implementation_roadmap: "Phased approach"
        - risks: "What could go wrong"

  # ---------------------------------------------------------------------------
  # COMMUNICATION_FABRIC: Specific schema for comm theory analysis
  # ---------------------------------------------------------------------------
  # USE WHEN: Analyzing or extending the Communication Fabric framework
  # COST: ~$0.80 (3 specialized runs)
  # TIME: ~150s
  # ---------------------------------------------------------------------------
  communication_fabric:
    description: "Analyze codebase through Communication Fabric lens"
    purpose: "Apply communication theory metrics and identify patterns"
    recommended_for:
      - "Measuring F, MI, N, SNR, R, ΔH"
      - "Identifying feedback loops"
      - "Detecting death spirals"
      - "Channel analysis"

    runs:
      - name: "metric_analysis"
        description: "Calculate communication metrics for target"
        type: "internal"
        model: "gemini-2.0-flash-001"
        tier: "long_context"
        sets: ["pipeline", "agent_intelligence"]
        token_budget: 150000
        temperature: 0.1
        system_prompt: |
          Analyze through Communication Fabric lens. For the target, estimate:

          METRICS:
          - F (Feedback Latency): Time from trigger→detect→correct→verify
          - MI (Mutual Information): Alignment between code and docs
          - N (Noise): Orphans, phantoms, drift, open loops
          - SNR (Signal-to-Noise): Closed loops / total events
          - R_auto (Automated Redundancy): Tests, types, schemas coverage
          - ΔH (Change Entropy): Recent structural novelty

          RISK ASSESSMENT:
          risk ≈ f(ΔH, centrality, 1/R_auto, F)

          Cite specific evidence for each metric estimate.

      - name: "loop_analysis"
        description: "Identify feedback loops and their health"
        type: "internal"
        model: "gemini-2.5-pro"
        tier: "long_context"
        sets: ["agent_kernel", "pipeline", "theory"]
        token_budget: 150000
        temperature: 0.2
        system_prompt: |
          Identify FEEDBACK LOOPS in the system:

          For each loop:
          1. NAME: Descriptive identifier
          2. TYPE: Negative (stabilizing) or Positive (amplifying)
          3. PHASES: trigger → detect → propose → implement → verify
          4. LATENCY: Estimated time per phase
          5. HEALTH: Is it closing? Open? Oscillating?

          DETECT DEATH SPIRALS:
          - F↑ → N↑ → SNR↓ → F↑ patterns
          - Identify if damping (R_auto) is sufficient

          DETECT VIRTUOUS CYCLES:
          - R_auto↑ → F↓ → N↓ → SNR↑ patterns

      - name: "channel_mapping"
        description: "Map communication channels"
        type: "internal"
        model: "gemini-2.0-flash-001"
        tier: "long_context"
        sets: ["pipeline", "schema", "agent_specs"]
        token_budget: 100000
        temperature: 0.1
        system_prompt: |
          Map COMMUNICATION CHANNELS in the system:

          For each channel:
          1. ID: Unique identifier
          2. SOURCE: What generates messages
          3. RECEIVER: What consumes messages
          4. MEDIUM: File system, API, Git, etc.
          5. MESSAGE_TYPES: What payloads flow through
          6. CAPACITY: Estimated throughput
          7. NOISE_SOURCES: What degrades the channel

          Identify BOTTLENECKS where capacity limits flow.

    synthesis:
      strategy: "triangulation"
      output_format: "structured"
      fields:
        - metrics_summary: "F, MI, N, SNR, R, ΔH estimates"
        - feedback_loops: "Identified loops with health status"
        - channels: "Communication channel map"
        - death_spiral_risk: "Current risk assessment"
        - recommended_actions: "Priority interventions"
        - loop_closure_opportunities: "Open loops to close"

  # ---------------------------------------------------------------------------
  # QUICK_VALIDATE: Fast 2-run validation (cost-optimized)
  # ---------------------------------------------------------------------------
  # USE WHEN: Need quick validation without full trio
  # COST: ~$0.15 (2 cheap runs)
  # TIME: ~30s
  # ---------------------------------------------------------------------------
  quick_validate:
    description: "Fast validation with minimal cost"
    purpose: "Quick sanity check for simple claims"
    recommended_for:
      - "Simple existence checks"
      - "Count verification"
      - "Location queries"

    runs:
      - name: "primary"
        type: "internal"
        model: "gemini-2.0-flash-001"
        sets: ["theory"]
        token_budget: 30000
        temperature: 0.1

      - name: "verify"
        type: "internal"
        model: "gemini-2.0-flash-001"
        sets: ["pipeline"]
        token_budget: 30000
        temperature: 0.1
        system_prompt: "Verify the following claim with code evidence."

    synthesis:
      strategy: "consensus"
      min_agreement: 2
      distinct_sources_required: true
      output_format: "brief"

  # ---------------------------------------------------------------------------
  # FOUNDATIONS: Reference library for intellectual foundations
  # ---------------------------------------------------------------------------
  # USE WHEN: Asking about theoretical underpinnings of SMoC
  # COVERAGE: 82 academic works, 5.4M tokens, 856 captioned figures
  # ---------------------------------------------------------------------------
  foundations:
    description: "Query the intellectual foundations reference library"
    purpose: "Trace SMoC concepts to academic sources"
    recommended_for:
      - "Theoretical questions about SMoC origins"
      - "Finding academic citations"
      - "Understanding foundational concepts"

    runs:
      - name: "primary"
        type: "internal"
        model: "gemini-3-pro-preview"
        sets: ["foundations"]
        token_budget: 500000
        temperature: 0.2
        system_prompt: "You have access to the complete SMoC reference library with 82 academic works. Cite specific references (REF-XXX) and quote relevant passages."

    synthesis:
      strategy: "direct"
      output_format: "markdown"

# =============================================================================
# EXTERNAL MEMBRANE ENFORCEMENT
# =============================================================================
# Runs with type: "external" have these rules ENFORCED by the loader:
#
# 1. sets: IGNORED (external queries don't use repo context)
# 2. token_budget: IGNORED (no context to budget)
# 3. system_prompt: SANITIZED (no "cite file:line" allowed)
# 4. Query: Passed through prepare_perplexity_query() ALWAYS
#
# Violation = loader error, not silent override.

external_membrane:
  strict: true
  banned_in_external_prompts:
    - "file:line"
    - "cite specific code"
    - "reference the repository"
    - "look in the codebase"

# =============================================================================
# GUARDRAILS
# =============================================================================

guardrails:
  max_token_budget_per_run: 1000000
  max_token_budget_per_schema: 2000000
  max_runs_per_schema: 10
  rate_limit_per_minute: 30
  cost_alert_threshold_usd: 1.00

# =============================================================================
# AI AGENT QUICK REFERENCE
# =============================================================================
#
# SCHEMA SELECTION GUIDE:
# -----------------------
# | Need                          | Use Schema            |
# |-------------------------------|-----------------------|
# | High confidence answer        | validation_trio       |
# | Optimize token budget         | depth_ladder          |
# | Test hypothesis robustness    | adversarial_pair      |
# | Debug complex issue           | forensic_investigation|
# | Precise confidence score      | confidence_calibration|
# | Explore across scale levels   | semantic_probe        |
# | Quick sanity check            | quick_validate        |
# | Mine Claude Code history      | claude_history_ingest |
# | Build knowledge graph         | mind_map_builder      |
# | Develop/validate frameworks   | theoretical_discussion|
# | Communication Fabric analysis | communication_fabric  |
#
# PARAMETER OVERRIDE SYNTAX:
# --------------------------
# python analyze.py --research validation_trio \
#   --override "runs[0].token_budget=200000" \
#   --override "runs[1].sets=['pipeline','schema']" \
#   "Your query here"
#
# CUSTOM SCHEMA SYNTAX:
# ---------------------
# python analyze.py --research-custom '{
#   "runs": [
#     {"name": "a", "model": "gemini-3-pro-preview", "type": "internal", "sets": ["theory"]},
#     {"name": "b", "type": "external", "tier": "perplexity"}
#   ],
#   "synthesis": {"strategy": "consensus", "distinct_sources_required": true}
# }' "Your query here"
#
# OUTPUT FORMATS:
# ---------------
# --output-format structured  (default: YAML)
# --output-format json
# --output-format markdown
# --output-format brief
#
# CONDITION DSL:
# --------------
# scope_in: ["EXTERNAL", "HYBRID"]   # Scope must be one of these
# scope_not: "INTERNAL"               # Scope must NOT be this
# complexity_gte: 3                   # Complexity >= threshold
# intent_in: ["EXPLAIN", "ANALYZE"]   # Intent must match
