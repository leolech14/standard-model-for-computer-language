#!/usr/bin/env python3
"""Validate all metadata against library_schema.json."""

import json
import jsonschema
from pathlib import Path
import sys

REFS_DIR = Path(__file__).parent.absolute()
SCHEMA_FILE = REFS_DIR / "library_schema.json"
METADATA_DIR = REFS_DIR / "metadata"


def validate_single(ref_id: str) -> tuple[bool, list[str]]:
    """Validate one metadata file. Returns (is_valid, errors)."""
    meta_file = METADATA_DIR / f"{ref_id}.json"
    if not meta_file.exists():
        return False, [f"File not found: {meta_file}"]

    try:
        schema = json.loads(SCHEMA_FILE.read_text())
        metadata = json.loads(meta_file.read_text())

        # Validate against schema
        jsonschema.validate(instance=metadata, schema=schema)

        # Additional checks
        errors = []

        # Check file paths exist
        if metadata.get("original_file"):
            pdf_path = REFS_DIR / metadata["original_file"]
            if not pdf_path.exists():
                errors.append(f"PDF not found: {metadata['original_file']}")

        if metadata.get("enhanced_txt"):
            txt_path = REFS_DIR / metadata["enhanced_txt"]
            if not txt_path.exists():
                errors.append(f"TXT not found: {metadata['enhanced_txt']}")

        if metadata.get("images_dir"):
            img_dir = REFS_DIR / metadata["images_dir"]
            if not img_dir.exists():
                errors.append(f"Images dir not found: {metadata['images_dir']}")

        # Check for stub placeholders
        if metadata.get("summary") == "[TO BE GENERATED BY LLM]":
            errors.append("Summary is still a stub placeholder")

        if metadata.get("smoc_relevance_summary") == "[TO BE GENERATED BY LLM]":
            errors.append("SMoC relevance is still a stub placeholder")

        # Check cross-references resolve
        for cross_ref in metadata.get("cross_references", []):
            ref_file = METADATA_DIR / f"{cross_ref}.json"
            if not ref_file.exists():
                errors.append(f"Cross-reference broken: {cross_ref}")

        return len(errors) == 0, errors

    except jsonschema.ValidationError as e:
        return False, [f"Schema validation failed: {e.message}"]
    except json.JSONDecodeError as e:
        return False, [f"Invalid JSON: {e}"]
    except Exception as e:
        return False, [f"Unexpected error: {e}"]


def validate_all() -> dict:
    """Validate all metadata files."""
    results = {
        "total": 0,
        "valid": 0,
        "invalid": 0,
        "warnings": 0,
        "errors": []
    }

    meta_files = [f for f in METADATA_DIR.glob("*.json") if not "_analysis" in f.name]

    for meta_file in sorted(meta_files):
        ref_id = meta_file.stem
        results["total"] += 1

        is_valid, errors = validate_single(ref_id)

        if is_valid:
            results["valid"] += 1
            print(f"✓ {ref_id}")
        else:
            # Check if errors are just warnings (stubs)
            only_stubs = all("stub placeholder" in e for e in errors)

            if only_stubs:
                results["warnings"] += 1
                print(f"⚠ {ref_id} (awaiting LLM analysis)")
            else:
                results["invalid"] += 1
                print(f"✗ {ref_id}")
                for error in errors:
                    print(f"    {error}")
                results["errors"].append({"ref_id": ref_id, "errors": errors})

    return results


def main():
    print("=" * 80)
    print("METADATA VALIDATION")
    print("=" * 80)
    print()

    if "--all" in sys.argv or len(sys.argv) == 1:
        results = validate_all()

        print()
        print("=" * 80)
        print("RESULTS")
        print("=" * 80)
        print(f"Total:    {results['total']}")
        print(f"Valid:    {results['valid']}")
        print(f"Warnings: {results['warnings']} (stub placeholders)")
        print(f"Invalid:  {results['invalid']}")
        print()

        if results['warnings'] > 0:
            print(f"⚠ {results['warnings']} refs awaiting LLM analysis (Phase 2)")
            print("  Run: python3 analyze_ref.py <REF-ID>")

        if results['invalid'] > 0:
            print(f"\n✗ {results['invalid']} refs have validation errors")
            return 1

        return 0

    else:
        # Validate single ref
        ref_id = sys.argv[1]
        is_valid, errors = validate_single(ref_id)

        if is_valid:
            print(f"✓ {ref_id} is valid")
            return 0
        else:
            print(f"✗ {ref_id} has errors:")
            for error in errors:
                print(f"  - {error}")
            return 1


if __name__ == "__main__":
    sys.exit(main())
